- What's kubernetes: container orchestration tool

  - Problem solved:
    - `microservices deployment`: manange many(thousands of) containerized microservices
    - Auto Scaling, Health Monitoring/Auto-Respawn, Load Balancing, Service Discovery, Version Control
    - Cloud Agnostic
  - what features to offer?
    - `high availability`, no downtime
    - `scalability` (scale in/out)
    - `disaster recovery`

- [doc](https://kubernetes.io/docs/home/)

  - [ref](https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests)
  - [bili](https://www.bilibili.com/video/BV1Qv41167ck?p=2)
    - [集群搭建](https://www.bilibili.com/video/BV1Qv41167ck?p=5)
  - [helm](https://www.youtube.com/watch?v=kJscDZfHXrQ)

- tools
  - [minikube](https://minikube.sigs.k8s.io/docs/start/): 快速搭建单主节点集群工具。
  - [kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/):快速搭建多主节点集群工具。

# tldr

- Kubernetes == Cluster Nodes(running Kubernetes processes, and containers)
- Abstract Resources

  - `Pods`: deployable application, can be 1 or more container
  - `ReplicaSets`: declare that: specified number of pod always running (through Replica Controller)
  - `Deployment`: abstraction over ReplicaSets. Achieve typical deployment tasks
  - `Service`: expose service to outside cluster, achieve service discovery & load balancer (layer 4, TCP/UDP).
    - `ClusterIP`(default): create an permant in-cluster serviceIP to a service
      - `port:target-port`: (serviceIP:port => load balance => pod:target-port)
      - `Headless`: a service without ClusterIP and loadbalancing, can only be access through Domain name: commonly used to expose `stateful apps`
        - by setting `clusterIP: None` in a ClusterIP config
    - `NodePort`: expose a port from any k8s node to a service
      - `NodePort:port:target-port`: (anyNodeIp:NodePort => serviceIP:port => load balance => pod:target-port)
    - `LoadBalancer`: build upon NodePort,使用外部云环境的负载均衡器来完成 Request 到 Node:NodePort 的负载均衡。
    - `ExternalName：` 把集群外部的 service 引入到集群，可以从集群内部访问
    - domainName template: `http://<service-name>.<namespace>.svc.cluster.local:<service-port>`
  - `endpoints`: a list of service's target pods addresses. generated from selector and stored in etcd
  - `coredns`
  - `Ingress`: layer 7 (HTTP) reverse-proxy: rule-based routing, load balancer => expose ClusterIP service to outside as domainNames
    - provide a list of mapping: domainName => service(ClusterIP). route HTTP request based on the mapping.
  - `configMap/secret`: Parameter/Secret Storage
  - `volumn`: Data Persistence
  - `Statefulset`: for deploy stateful application
  - `Namespace`: isolate resource for different purpose
  - `Label`: key-value pair, used to group pods (define pods that an config applies to)

    - label-resource: many to many relation

  - `Controllers`: utilities components that helps implement various Resources (like replicas). Setting `config parameters` in Controller Manager can modify these controller's behavior

- Physical Components
  - master nodes:
    - `API Server`: communication hub for everything: admin access, write to ETCD, communicate to kubelets(master-worker, worker-worker)
    - `ETCD`: distributed key-value store for saving k8s cluster state (think the database of K8S backend)
    - `Controller Manager`: `Controllers` ensure the cluster's desired state is maintained. `Controller Manager` manage different controllers
      - `Node Controller`: monitor worker's health status, unregister unreachable node and provision replacement
      - `ReplicaSets`: (old: Replica Controller) ensure #pod/replicas are available all times
      - Namespace Controller
      - Endpoint Controller
      - Deployment Controller
    - `Scheduler`: decide which node a new pod will be deployed
  - worker nodes:
    - `kubelet`: receive instruction from master, manage pods in node, report to master
    - `kube-proxy`: a network interface and outbound load balancer
    - `Container Runtime Interface(CRI)`: run container

# Details(Basic, 略)

## Abstract Resource

### `Pods`: an single Application Instance

- pod are the `smallest (basic) deployable element`, an `abstration of an application`.

  - an abstraction over containers: a pod could contains `multiple containers`
    - general rule: one container = one process.
    - multi-containers pod: containers shares
      - IP address, share the same localhost and can communicate directly.
      - docker volumn
      - same lifecycle
      - use when: two application are very close coupled and must share the same lifecycle
  - each pod gets `one temporary private IP address` in the virtual network
    - `ephemeral`: once a pod die and a new pod replace it, with a `new temporary private IP`

### `ReplicaSets(apps/v1)` (old: `Replication Controller(v1)`): make replicas of pods

- ReplicaSets ensures that: `specified number of pod` always running in the cluster

  - Provide Pod with high availability, horizontal scaling, Health Monitoring/Auto-Respawn, and Load Balancing
  - ReplicaSets is managed by `Replica Controller`

- Api versions

  - `replication Controllers` are part of the older generation API(`v1`)
  - `ReplicaSets` are part of the newer generation API(`apps/v1`), and more features

### `Deployment`: Achieve typical `Deployment Tasks`

- `Deployment`: Achieve typical `Deployment Tasks` like

  - deploy high available app (through replicas)
  - rolling updates
  - rollback changed
  - pause state and resume

- `Deployment` is an abstraction over `ReplicaSets`: deploy a `Deployment` Automatically creates a `ReplicaSets` and the underlineing `pods`

### `Service`: perment IP + Load Balancer

- Service: expose service to outside cluster, achieve service discovery & load balancer

  - It is implemented in a cluster way: through `Kube-proxy` in all work nodes
  - used by `other microservice` or `client(from external network)` to access an microservice
  - service is `attachable abstract resource`.

- Service Types

  - `ClusterIP(default)`: in-cluster IP
  - `NodePort`: external port
  - `LoadBalancer`: third-party LoadBalancer
  - ExternalName
  - HeadLiness

- Load Balancing:

  - if sessionAffinity:ClientIP, use sessionAffinity based on client IP, with TTL
  - if undefined, use strategy defined by kube-proxy (round-robin/ random...)

- Domain name:
  - <service-name>.<namespace>.svc.cluster.local:<service-port> (eg: http://my-clusterip-name.default.svc.cluster.local:9200)
  - find domain server and manually resolve

```bash
# from any node:
cat /etc/resolv.conf   # nameserver 172.27.96.1
dig @172.27.96.1 <domain-name>
```

#### `Service: ClusterIP`: Internal Communication

- `ClusterIP`: create a permenant `vIP(serviceIP)` inside cluster for in-cluster microservice to communicate

  - permanent IP, load balanced route to resource

- Components

  - `Port(Service Port): 80`: port of service (a virtual cluster IP): request send to 80 of service IP
  - `TargetPort:80`: 转发到 port 80 of a pod

- `Headless`: a service without ClusterIP and loadbalancing, can only be access through Domain name
  - how: setting `clusterIP: None` in a ClusterIP config
  - usage:
    - create Stateful service (databases, MQ/Kafka)

#### `Service: NodePort`: Expose Service to outside

- `NodePort`: `expose an port in any k8s node` to a service

- `NodePort:port:target-port`: (anyNodeIp:NodePort => serviceIP:port => load balance => pod:target-port)

- Components

  - `NodePort`: a port in k8s nodes,randomly assigned by default(range `30000-32767`)
  - `Port(Service Port)`: port of service (a virtual cluster IP) : request send to 80 of service IP
  - `TargetPort`: 转发到 port 80 of a pod

#### `Service: LoadBalancer`: external load balancer

- build upon NodePort,使用外部云环境的负载均衡器来完成 Request 到 Node:NodePort 的负载均衡。

#### `Service: ExternalName`: use external (outside k8s cluster) service in cluster

- 注册中心: 注册外部服务(IP/DomainName)为内部服务

### Namespace

- Namespace: for isolate resource for different purpose

- namespace types

  - `default` Namespace: without specification, you use the `default namespaces`
  - `kube-system`: used for cluster-level resources and system components.
  - `kube-node-lease`: managed by Kubernetes and is used to maintain the health of worker nodes status
  - `kube-public`: considered a `read-only namespace`. It's often used for resources (eg, ConfigMaps) that should be available and read-only to all users in the cluster
  - `custom namspaces`: eg, `dev`, `prod`

- use namespace:

  - component within namespace refers each other by `<resource-name>`
  - component refers components in other namespace by `<resource-name>.<namespace>.svc.cluster.local`

- `ResourceQuota`: limit hardware resource a namespace can use

### `Ingress`: think reverse proxy at HTTP level (layer 7)

- [ref](https://kubernetes.io/docs/concepts/services-networking/ingress/)
- [bili](https://www.bilibili.com/video/BV1Qv41167ck?p=68)

- ingress: define `rules` to route HTTP traffic to Pod or Service: routing + load balancing

  - expose Service to outside of cluster, alternative way from NodePorts

- Ingress Controller: actual implementation to ingress: can be `Nginx`, Contour, Haproxy...

```bash
minikube addons enable ingress
```

- 工作原理
  - deploy Ingress to k8s: a Nginx Pod, and expose a NodePort service ingress-nginx
  - define `Ingress Rules`: which domainName => which Service (ClusterIP)
  - Ingress Controller detect Ingress Rules, then generate and execute a Nginx configuration

### `configMap/secret`: Parameter/Secret Storage

- `configMap`: save `unsensitive` info (eg, database url)
- `secret`: save `sensitive` info (userName, pwd)

- pods access to configMap/secret

  1. manage/setting configMap/secret in k8s admin
  2. connect `configMap/secret` to a `pod`
  3. use params in pod as `environment variables` or as a `properties file`

### `volumn`: Data Persistence

- `Volumn` persist database pod's data to a storage place

  - Why: When a database pod die, data inside it is gone.
  - How:
    - A volumn is associated with a physical storage location(cloud or on-premise)
    - Data from pod are persist into volumn
    - `When a database sevice pod die and restarted, these data are automatically restored to the restarted service/pod`

### `Statefulset`

- Statefulset is for deploy `stateful application` (eg, a database, SQL, nonSQL, elastiSearch...)

  - issue: multiple DB pod connect to same date volumn create `competition`
  - Statefulset is used specificly for creating `stateful application`
  - Statefulset makes sure that multiple pods' write to a database is `Synchronized` and safe.

- A common practice is to `not host DBs inside kubernetes` cluster.

  - managing stateful container in kubernetes is complex
  - container runtime creates performance bottleneck

- Use cloud solution or on-premise database is more common
  - easier to achieve: Master-Slave, Multi-Region, High Availability, Sharding, DR, etc.

## Kubernetes Cluster

- `node`: virtual or physical machine
- `Kubernetes Cluster` = `master nodes` + `worker nodes`
  - worker nodes runs:
    - `kubelet`: receive instruction from master, manage pods lyfecycle in node, report status to master
    - `kube-proxy`: collectively contribute to service discovery and and load balancing in cluster (service's interface)
    - `Container Runtime Interface(CRI)`: run container
  - master nodes runs:
    - `API Server`: communication hub for everything: admin access, write to ETCD, communicate to kubelets
    - `ETCD`: distributed key-value store for saving k8s cluster state (database of K8S backend)
    - `Controller Manager`: ensure the cluster's desired state is maintained: desired number of replicas, (by trigger scheduler)
    - `Scheduler`: decide which node a new pod will be deployed

### Control(master) Nodes

- `Control(master) node`:

  - schedule deployment
  - monitor cluster performance
  - register/delete work nodes

- `master node replicas`: in production it has at least 1 backup of the master node in case of breakdown

#### `Kube API Server (aka API Server)`: Communication hub for everything

- expose endpoint for `admin tasks`(UI, API, CLI). Also includes an `authentication ststem`

  - receieve and execute the user config file (.yaml)
  - register new master/worker node to a cluster
  - more

- an interface for internal communication

  - Allow `workers to communicate` with master through kubelet
  - API server is the primary component that write to etcd
  - `Controller Manager, Scheduler, and etcd` also communicate through API server

- example: create pod in API Server
  - user send create pod request
  - API Server authenticate user and vaildate request
  - API Server create pod record, update information in etcd
  - Scheduler discovered the unassigned pod record, then assign a worker node for the pod, send update to the API Server
  - API Server update etcd for node association, and send request to the kubelet of the worker node
  - kubelet create the pod, create containers in CRI, then report back to API Server
  - API Server update etcd for the creation

#### `Controller Manager`: Monitor and recover

- `Controller Manager` ensure cluster's desired state is maintained

- `Controller Manager` manage many different `controllers programs` in k8s (think plugins), like

  - `Node Controller`: ensure the nodes are healthy
    - monitor worker's health status changes every x second
    - unregister unreachable node and provision replacement
  - `ReplicaSets (Replica Controller)`: ensure #pod/replicas are available all times
    - monitor #pod meet the desirable replicas requirement
    - add pods is pods die\
  - Ingress Controller
  - namespace Controller
  - Endpoint Controller
  - Deployment Controller
  - ...

- In `Controller Manager` you set `config parameters` for each controller (like monitor frequency...)

#### `Scheduler`: decide `which node a new pod will be deployed`

- `Scheduler`

  - does not create pod or deploy pod to node (API Server do that).
  - `only decides the node assignment of new pod`

- How to decide:
  - filter out node without available resource (CPU, RAM...)
  - Calculate the best fit through a ranking function, incl factor like
    - user constraint: eg, certain nodes for certain applications
    - PriorityClasses,
    - etc

#### `ETCD`: etcd is `a distributed key-value store (cluster)`. Serves as the cluster's primary `database`.

- storing `k8s cluster state (aka configuration data)` at a point in time
  - info about nodes, pods, services, namespaces, and more
  - once created these data are immutable
- data can be used for
  - `Scheduler/Controller Manager` as a reference to perform their task
  - `backup, audit and restore`
- `Data Security` required: TLS communication, access authentication...
- etcd data is typically persist on disk

### Workers nodes

- `worker node`: host application as containers
- `a work node` would typically `run multiple pods`. This helps maximize resource utilization on the cluster.

#### `kubelet`: manage node. talk to both OS and the API Server

- Kubelet is bridge between

  - the OS and the Container Runtime Interface (CRI):
    - start/stop containers
  - the `Kubernetes API Server` process on the master node
    - register a node to the k8s cluster
    - follow deployment instruction to spawn/stop pods.
    - in-node `Pod Monitoring`, and regularly Report Node Status to master

#### `Kube-proxy`: `a node's network interface`

- [ref](https://www.bilibili.com/video/BV1Qv41167ck?p=58)

- `load balancer` + `sticky sessions`

  - `load balancer`: Kube-proxy includes a load balancer `for reaching a service from the node`:
  - `sticky sessions`: Kube-proxy can also provide session affinity (sticky sessions) management

- three implementations modes:

  - userspace
  - iptables (default if ipvs not installed)
  - ipvs (preferred, need to config ipvs in system)

- `Service`: Kube-proxy maintain a `local proxy table` created by ipvs,
  - which maps serviceIp:port => many podIp:targetPort
  - and load balanced route to the correct pod through the table

#### `Container Runtime Interface(CRI)`: official docker runtime in k8s is `containerd`. `Run images`.

- official docker runtime for k8s is `containerd`, docker is not longer supported after v1.24. (can still use docker built image)

- `containerd` is a `Minimized Container Runtime`, ie it cut unnecessary function to run a service in production
  - just for running containers
  - `not incl`: cli, api, build tools, volumes, auth/security...
