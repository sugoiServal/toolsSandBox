- What's kubernetes: container orchestration tool, for connecting and managing application that makes up of a large scale of container nodes(virtual, physical, hybrid, microservices...)
  - typical use case: `microservices`
- what features to offer?
  - `high availability`, no downtime
  - `scalability` (increasing or decreasing)
  - `disaster recovery`
- what problem it solve:

  - manange many(thousands of) containerized microservices

- [ref](https://www.youtube.com/watch?v=X48VuDVv0do)
- [doc](https://kubernetes.io/docs/home/)

# Abstract Components of K8s cluster

## Application Layer

### `Pods`: an single Application Instance

- pod are the `smallest (basic) deployable element`. It is an `abstration of an application`.

  - it is layer(a runtime) abstract over container
  - each pod gets `one temporary private IP address` in the virtual network
  - pods are `ephemeral`, once a pod die and a new pod replace it with a `new IP address`

- a pod could contains `multiple containers`
  - Do this only when multiple containers are necessary to comprise one application
  - Also, follow the rule: one container = one process
  - When a Pod runs `multiple containers`, the containers share the Pod's resources, incl
    - IP address, share the same localhost and can communicate directly.
    - same local storage (docker volumn)
    - same lifecycle
- `Replication`: pod are usually `replicated` for `high availability` or `horizontal scaling`

### `Service`: perment IP + Load Balancer

- service is `perment IP address` that can be `attached to pods`

  - when a pod die and replaced, the service got detached and then attached to the new pod, hense a static IP

- external vs internal

  - `external service (public)`: service that can be exposed to the outside
  - `internal service (private)`: service that cannot be exposed to the outside

- Pods within Service are `load balancing` through Kube-proxy

  - you create or replicate Pods though `deployment`
  - attach new Pods to a Service, hence `the pod belongs to a service`
  - when users hits a service, the requests are balanced among all healthy pods

- Service is `not a process` in K8S, it is `abstract resource !!!`

  - Services act as a network abstraction layer, enabling `stable pods communication regardless of their underlying placement`.
  - `Kube-proxy` helps implement this abstraction!!

- the process breakdown
  1. create a Kubernetes Service (through yaml)
  2. Get Service IP: When the Service is created, Kubernetes assigns it a cluster IP address (aka: Virtual IP)
  3. Kube-proxy `local proxy table`: Kube-proxy maintains a set of "endpoints". A big table of services (Virtual IP) available and the IP/ports of the pods belongs to it
  4. Kube-proxy `continuously monitors` the Kubernetes API server for new Services/ pods association
  5. Kube-proxy `load balancing`: when a request to a service is sent, Kube-proxy load balancing the request to a pod within the service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app # set of pod that belongs to the service
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```

### `ingress`: think reverse proxy at HTTP level

- ingress

  - handler of client traffic (HTTP endpoint to a service)
  - define `rules` to route HTTP traffic to `Ingress resource`: Pod/Service
  - forward requests to services

### `configMap/secret`: Parameter/Secret Storage

- `configMap`: save unsensitive info (eg, database url)
- `secret`: save sensitive info (userName, pwd)

- pods (aka application codes) will access configMap/secret

  1. connect `configMap/secret` to a `pod`
  2. use params in pod as `environment variables` or as a `properties file`

- usage
  - manage (setting) configMap/secret in k8s admin
  - you may want to encode your secret string(eg, base64) before store them in secret

### `ReplicaSets` (old: `Replication Controller`)

- why replication: high availability, load balancing

- `Replication Controllers` vs `ReplicaSet`

  - `replication Controllers` are part of the older generation (`v1`)
  - `ReplicaSets` are part of the newer generation(`apps/v1`), more features and flexibility

- Replication is achieved through the `ReplicaSets` (managed by `Controller Manager`)

  - `ReplicaSets` ensures that: `specified number of pod` always running in the cluster

- ReplicaSets affect pods through both:
  - `selector` and `template`

## Data Layer

### `volumn`: Data Persistence

- `Volumn` persist database pod's data to a storage place

  - Why: When a database pod die, data inside it is gone.

- How:

  - A volumn is associated with a physical storage location(cloud or on-premise)
  - Data from pod are persist into volumn
  - When a database sevice pod die and restarted, these data are automatically restored to the restarted service/pod

- it's the responsibility of the users to use volumes to backup their data

## Deployment

### `Deployment`

- A `Depolyment` is how k8s user deploy their `statefulless application` to actual Pod & Service

  - declarative: eg. expect 2 replica pod, then k8s will try the best to maintain 2 pods (scale out/in)

### `Statefulset`

- Statefulset is for deploy `stateful application` (eg, a database, SQL, nonSQL, elastiSearch...)

  - issue: multiple DB pod connect to same date volumn create `competition`
  - Statefulset is used specificly for creating `stateful application`
  - Statefulset makes sure that multiple pods' write to a database is `Synchronized` and safe.

- A common practice is to `not host DBs inside kubernetes` cluster
  - managing stateful container in kubernetes is complex
  - container runtime creates performance bottleneck
  - use cloud solution or on-premise database is more common.
    - easier to achieve the desired: Master-Slave, Multi-Region, High Availability, Sharding, DR, etc

### Namespace

# Kubernetes Cluster

- `node`: virtual or physical machine
- `Kubernetes Cluster` = `master nodes` + `worker nodes`
  - worker nodes runs:
    - `kubelet`: receive instruction from master, manage pods in node, report to master
    - `kube-proxy`: a network interface and outbound load balancer
    - `Container Runtime Interface(CRI)`: run container
  - master nodes runs:
    - `API Server`: communication hub for everything, admin access, write to ETCD
    - `ETCD`: distributed key-value store for saving k8s cluster state (database of K8S backend)
    - `Controller Manager`: ensure the cluster's desired state is maintained(trigger scheduler)
    - `Scheduler`: decide which node a new pod will be deployed

## Control(master) Nodes

- `Control(master) node`:

  - schedule deployment
  - monitor cluster performance
  - register/delete work nodes

- `master node replicas`: in production it has at least 1 backup of the master node in case of breakdown

### `Kube API Server (aka API Server)`: Communication hub for everything

- expose endpoint for `admin tasks`(UI, API, CLI). Also includes an `authentication ststem`

  - receieve and execute the user config file (.yaml)
  - register new master/worker node to a cluster
  - more

- an interface for internal communication

  - Allow `workers to communicate` with master through kubelet
  - API server is the primary component that write to etcd
  - `Controller Manager, Scheduler, and etcd` also communicate through API server

- example: create pod in API Server
  - user send create pod request
  - API Server authenticate user and vaildate request
  - API Server create pod record, update information in etcd
  - Scheduler discovered the unassigned pod record, then assign a worker node for the pod, send update to the API Server
  - API Server update etcd for node association, and send request to the kubelet of the worker node
  - kubelet create the pod, create containers in CRI, then report back to API Server
  - API Server update etcd for the creation

### `Controller Manager`: Monitor and recover

- `Controller Manager` ensure cluster's desired state is maintained

- `Controller Manager` manage many different `controllers programs` in k8s (think plugins), like

  - `Node Controller`: ensure the nodes are healthy
    - monitor worker's health status changes every x second
    - unregister unreachable node and provision replacement
  - `ReplicaSets (Replica Controller)`: ensure #pod/replicas are available all times
    - monitor #pod meet the desirable replicas requirement
    - add pods is pods die
  - namespace Controller
  - Endpoint Controller
  - Deployment Controller
  - ...

- In `Controller Manager` you set `config parameters` for each controller (like monitor frequency...)

### `Scheduler`: decide `which node a new pod will be deployed`

- `Scheduler`

  - does not create pod or deploy pod to node (API Server do that).
  - `only decides the node assignment of new pod`

- How to decide:
  - filter out node without available resource (CPU, RAM...)
  - Calculate the best fit through a ranking function, incl factor like
    - user constraint: eg, certain nodes for certain applications
    - PriorityClasses,
    - etc

### `ETCD`: etcd is `a distributed key-value store (cluster)`. Serves as the cluster's primary `database`.

- storing `k8s cluster state (aka configuration data)` at a point in time
  - info about nodes, pods, services, namespaces, and more
  - once created these data are immutable
- data can be used for
  - `Scheduler/Controller Manager` as a reference to perform their task
  - `backup, audit and restore`
- `Data Security` required: TLS communication, access authentication...
- etcd data is typically persist on disk

## Workers nodes

- `worker node`: host application as containers
- `a work node` would typically `run multiple pods`. This helps maximize resource utilization on the cluster.

### `kubelet`: manage node. talk to both OS and the API Server

- Kubelet is bridge between

  - the OS and the Container Runtime Interface (CRI):
    - start/stop containers
  - the `Kubernetes API Server` process on the master node
    - register a node to the k8s cluster
    - follow deployment instruction to spawn/stop pods.
    - in-node `Pod Monitoring`, and regularly Report Node Status to master

### `Kube-proxy`: `a node's network interface`

- Ensure incoming traffic reach a `specific destination` in the node, typically pods running on the node.

- Ensure outbound traffic reaches the destination correctly (`load balancer`, `sticky sessions`)

  - `load balancer`: Kube-proxy also includes a load balancer `for reaching a service from the node`:
    - ensures outbound traffic to a Kubernetes service is distributed evenly among the pods
    - ensures that outbound traffic is sent to a healthy desitination
    - ensures that the network overhead to be the lowest
  - `sticky sessions`: Kube-proxy can also provide session affinity (sticky sessions) management

- `kube-proxy` `balance load` to `pods` within a `Service`. kube-proxy in the implemenation of `balance load`, and Service's is the abstraction of load balancing function.

### `Container Runtime Interface(CRI)`: official docker runtime in k8s is `containerd`. `Run images`.

- official docker runtime for k8s is `containerd`, docker is not longer supported after v1.24. (can still use docker built image)
- `containerd` is a `Minimized Container Runtime`, ie it cut unnecessary function to run a service in production
  - just for running containers
  - `no incl`: cli, api, build tools, volumes, auth/security...
