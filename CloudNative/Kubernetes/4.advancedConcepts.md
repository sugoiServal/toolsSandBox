# Labeling

- two ways to match labels

```yml
matchLabels:
  key: value

matchExpressions:
  - { key: app, Operator: In, values: ["v1", "v2"] }
```

# pods ~

- [bili](https://www.bilibili.com/video/BV1Qv41167ck?p=32)
- [Pods lifecycle](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/)
- [init container](<[ref](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/)>)

![](https://imgur.com/RerS3QH.jpg)

## Pods Resource

- `spec.containers.resources`

  - limits: 限制容器使用资源的上限, pod will be `terminate` if RAM exceed limit
  - requests: 设置启动容器的最小资源 (下限)

- [单位](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes)

  - ram: Mi, Gi, M, G
  - cpu:
    - 1 => 1 AWS vCPU, GCP Core, Azure Core...
    - 100m = 0.1

- Global Resource Control:
  - `LimitRange`: Enforce default limits & requests per Pod or Container in a namespace
  - `ResourceQuota`: limit aggregate resource consumption per namespace. (total number of objects, total compute resource)

```yml
# pod
spec:
  containers: # an array of containers
    - name: nginx
      image: nginx:latest
      ports:
        - containerPort: 80 # port a container open
      resources:
        limits:
          cpu: "2"
          memory: "10Gi"
        requests:
          cpu: "1"
          memory: "500Mi"
```

### `LimitRange`, `ResourceQuota`

```yml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
    - type: Container
      default: # this section defines default limits
        cpu: 500m
      defaultRequest: # this section defines default requests
        cpu: 500m
      max: # define the limit range (min, max)
        cpu: "1"
      min:
        cpu: 100m
    - type: Pod
      max:
        memory: "512Mi"
        cpu: "2"
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev # which namespace the quota applies to
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi
```

## Pods lifecycle

- Pod phase

  - `Pending`: pod 成功被创建，但是可能 pod 还没有被 schedule， 或者有容器还没有被 set up。
  - `Running`: 所有容器已经创建完成。
  - `Succeeded`: 所有容器成功退出。(eg, a batch job)
  - `Failed`: 所有容器已经退出,至少有一个不成功。
  - `Unknown`: For some reason the state of the Pod could not be obtained (eg, network issue)

- after the scheduler assigns a Pod to a Node, kubelet start creating container in container rumtime

- Container states

  - `Waiting`: neither Terminated or Running: for example, pulling the container image, or applying Secret data
  - `Running`: container process is executing. `postStart` hook runs before container enters Running state
  - `Terminated`: Either the container process completed or failed for some reason. `preStop` hook runs before container enters Terminated state (blocking container deletion)

### initContainers

- Init Containers run before the app containers (主容器) are started, for executing setup or preparation tasks for main containers

  1. Init containers can contain utilities or custom code for setup (eg, wget, git pull...)
  2. run code separate from the app containers. run code in main container can make an app container image less secure
  3. offer a mechanism to block or delay app container startup until a set of preconditions are met (eg. start Nginx and connect after making sure services is up).

- Init Containers feature

  - Init Containers belongs to a pod
  - `Init Containers must run until completion, otherwise pod will try to restart Init Containers, and main container will be blocked from start`
  - multiple Init Containers: only one Init Containers can be run at a time, Init Containers must be run in sequence (one completion signal the next to start)

```yml
# pod
spec:
  containers:
    - name: main-container
      image: nginx:latest
      # Add your main container configuration here

  initContainers:
    - name: init-wait-mySQL
      image: busybox:1.33.1
      command:
        [
          "sh",
          "-c",
          'until ping 192.168.109.201 -c 1 ; do echo "waiting for mysql..."; sleep 2; done;',
        ]
      # Add your init container configuration here

    - name: init-wait-redis
      image: alpine:3.14
      command:
        [
          "sh",
          "-c",
          'until ping 192.168.109.202 -c 1 ; do echo "waiting for redis..."; sleep 2; done;',
        ]
      # Add your init container configuration here
```

### lifecycle hooks

- lifecycle hooks

  - `postStart`
  - `preStop`

- `postStart`

  - start immediately after a container is started, block the main application process running (Containers' Running State) until finish.
  - usage: Initialization, Resource Health Check, Delaying Main Application Start...

- `preStop`

  - executed immediately before a container enters Terminated state (blocking container deletion)
  - usage: Data Persistence, Graceful Shutdown (Clean Up, Connection Draining)

- 4 types of action
  - `exec`: execute commands
  - `tcpSocket`: try to access a tcp socket, considered successful if the port is open
  - `httpGet`: try a Http GET request, considered successful if the response code >= 200 and < 400.
  - `grpc`: Performs a remote procedure call using gRPC. The target should implement gRPC health checks

```yml
spec:
  containers:
    - name: mycontainer
      image: nginx
      lifecycle:
        postStart:
          exec:
            command: ["/bin/sh", "-c", "echo Container is about to stop"]
        preStop:
          exec:
            command: ["/usr/sbin/nginx", "-s", "quit"]
---
lifecycle:
  postStart:
    exec:
      command: ["/bin/sh", "-c", "echo Container is about to stop"]
---
lifecycle:
  postStart:
    tcpSocket:
      port: 8080
---
lifecycle:
  postStart:
    httpGet:
      path: / # url address
      port: 8080
      host: 192.168.109.100 # host address
      scheme: HTTP # HTTP/HTTPS
```

### Container Probes: Health Check

- [ref](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)

- `Container Probes`: Container Probes is a diagnostic performed periodically by the `kubelet` (to make sure container is health). kubelet either executes code within the container, or makes a network request.

- Types of probe

  - `livenessProbe`: Indicates whether the container is running. (if not Success => container restart)
  - `startupProbe`: Indicates whether the application within the container is started. (if not Success => container restart)
  - `readinessProbe`: Indicates whether the container is ready to respond to requests. (if not Success => removed pod from service)

- probe outcomes: Success, Failure, Unknown

- 4 types of probe action
  - `exec`: execute commands
  - `tcpSocket`: try to access a tcp socket in container, considered successful if the port is listened
  - `httpGet`: try a Http GET request, considered successful if the response code >= 200 and < 400.
  - `grpc`: Performs a remote procedure call using gRPC. The target should implement gRPC health checks

```yml
spec:
  containers:
    - name: mycontainer
      image: nginx
      livenessProbe:
        httpGet:
          path: / # url address
          port: 8080
          host: 192.168.109.100 # host address
          scheme: HTTP # HTTP/HTTPS
        initialDelaySeconds: int # 容器启动后多少秒开始第一次探测
        timeoutSeconds: int # 每次探测等待超时时间
        periodSeconds: int # 执行探测的频率。10s default
        failureThreshold: int # 连续多少次探测失败，才被认为失败
        successThreshold: int # 连续多少次探测成功，才被认为成功, default: 1
---
livenessProbe: # success if command return 0
  exec:
    command: ["cat", "/tmp/healthy"]
---
livenessProbe: # success if socket can be created
  tcpSocket:
    port: 8080
---
livenessProbe: # success if status code 200 <= $1 < 400
  httpGet:
    path: / # url address
    port: 8080
    host: 192.168.109.100 # host address, localhost if not provide
    scheme: HTTP # HTTP/HTTPS
```

### Restart Policy

- restartPolicy
  - Always(default) 容器失效时自动重启。
  - OnFailure 容器退出且返回值不为 0 时重启
  - Never 无论如何都不重启
  - (阶梯重复重启策略: 重启的延迟时长随着重启次数递增: 10s, 20s, 40s, 80s, 160s, 300s, 300s..)

```yml
spec:
  containers:
    - name: nginx
      image: nginx:latest
      ports:
        - containerPort: 80 # port a container open
  restartPolicy: Never #
```

## Pods Scheduling

- [ref](https://www.bilibili.com/video/BV1Qv41167ck?p=38)

- Scheduling
  - kube-scheduler: (default) automatically Scheduling through kube-scheduler
  - Assigning:
    - `nodeName`, `nodeSelector`
  - Affinity and anti-affinity: `nodeAffinity`, `podAffinity`, `PodAntiAffinity`
  - Taints and Tolerations: `taints`, `toleration`

### Assigning

- `nodeName`: specify node name
- `nodeSelector`: specify nodes' label

```yml
spec:
  containers:
      - name: nginx
      image: nginx:latest
      ports:
        - containerPort: 80
  nodeName: node1      # hardcode a node
---
spec:
  nodeSelector:  # match a node's labels:
    nodeenv: dev   # schedule to node with label "nodeenv: dev"
```

### Affinity and Anti-affinity (难点)

- [overview](https://www.bilibili.com/video/BV1Qv41167ck/?p=40)
- [nodeAffinity](https://www.bilibili.com/video/BV1Qv41167ck/?p=41)
- [podAffinity](https://www.bilibili.com/video/BV1Qv41167ck/?p=42)
- [podAntiAffinity](https://www.bilibili.com/video/BV1Qv41167ck/?p=43)

- affinity.

  - nodeAffinity: 调度到符合条件的 node
  - podAffinity: 调度到符合条件的 pod 所处於的 topologyKey
  - PodAntiAffinity: 调度到符合条件的 pod 所不处于的 topologyKey

- general guide:

  - affinity: 如果 2 个应用频繁交互他们应该尽可能靠近。
  - AntiAffinity：多副本部署的应用应该把所有副本尽可能打散在不同 node 里。

- `requiredDuringSchedulingIgnoredDuringExecution`: hard assigning, same as nodeSelector
- `preferredDuringSchedulingIgnoredDuringExecution`

  - if resource that satisfing the rule exist, then pod go to the resource
  - if no such resource found, pod can also go to a node that not satisfy
  - weight:
    - there can be multiple preferred rules, each of them have a weight 1-100
    - larger weight rules is preferred

- `podAffinity.topologyKey`: depoly 到 matching pod 的 相同 node，相同网段(的 node)，还是相同 OS(的 node)?
  - topology.kubernetes.io/zone : 依照 网段
  - beta.kubernetes.io/os : 依照 os
  - "kubernetes.io/hostname" : node

```yml
# nodeAffinity: required
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: <NODE_LABEL_KEY>
                operator: In # key: Exists, DoesNotExist; value: In/NotIn,  Gt, Lt
                values: ["xxx", "yyy"]

---
# nodeAffinity: preferred
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100 # 1-100
        preference:
          matchExpressions:
            - key: <NODE_LABEL_KEY>
              operator: In
              values:
                - <NODE_LABEL_VALUE>
---
# podAffinity
affinity:
  podAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: <POD_LABEL_KEY>
                operator: In
                values:
                  - <POD_LABEL_VALUE>
          topologyKey: <TOPOLOGY_KEY> # !
---
# podAntiAffinity
affinity:
  podAntiAffinity: # !
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: <POD_LABEL_KEY>
                operator: In
                values:
                  - <POD_LABEL_VALUE>
          topologyKey: <TOPOLOGY_KEY>
```

### taints/tolerance

- `taints`: whether allows a node to be a target of scheduling
  - `PreferNoSchedule`: 尽量不要来除非没有别的 available node。
  - `NoSchedule`: 禁止调度，如果已经调度就不倒查了。
  - `NoExecute`: 禁止调度，已经调度的赶紧走。
- `tolerance`: allow a pod to be scheduled to a `tainted node`. tolerance does not guarantee that a pod is scheduled to the node it tolerate!

- cluster built with kubeadm will taint NoSchedule to all `master nodes`

- taint

```bash
k taint nodes <node-name> key=value:PreferNoSchedule   # add a PreferNoSchedule taint
kubectl taint nodes <node-name> key:PreferNoSchedule-   # remove a PreferNoSchedule taint
kubectl taint nodes <node-name> key:-   # remove all taint associate with key
```

- tolerance

```yml
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
  tolerations: # key=value:NoSchedule
    - key: "key"
      operator: "Equal" # Exist (key), Equal (key=value)
      value: "value"
      effect: "NoSchedule"
```

## Multi-Container Design Pattern

- [ref](https://www.xenonstack.com/insights/container-design-patterns-for-kubernetes)
- `Sidecar Pattern`: extend the function of a main application(parent).

  - The lifecycle of the sidecar is the same as the parent application
  - sidecar provide extends functionality (eg. sending log to ElasticSearch)

- `Ambassador Pattern`: acts as a service discovery layer.

  - The ambassador container contains all external services configuration, and maintain connection open. The application container invoke the ambassador container to use other service

- `Adapter Pattern`: keep the communication between containers consistent.
  - transforms the primary container's output into the output that fits our applications’ communication standards(between multiple services).

# Pod Controller (Workload Resource)

- [ref](https://kubernetes.io/docs/concepts/workloads/controllers/)
- [HorizontalPodAutoscaler ](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- Pod Controller(Workload Resource): intermediate layer that manages Pods. Maintain `replicas`. Use `template` to respawn pod.

| Workload Resource                 | api version | description                                                                               |
| --------------------------------- | ----------- | ----------------------------------------------------------------------------------------- |
| ReplicationController(deprecated) | v1          | replaced by ReplicaSet                                                                    |
| ReplicaSet                        | apps/v1     | basic function, maintain replicas, image update (ReplicationController(deprecated))       |
| `Deployment`                      | apps/v1     | based on ReplicaSet, to allow `version control`: rolling update, rollback                 |
| `HorizontalPodAutoscaler `        |             | based on Deployment, automatically scaling a workload resource (replicas) to match demand |
| `Job/Cronjob`                     | batch/v1    | one-time or regular tasks (eg. regular backups, report generation). Delete after finish.  |
| `DaemonSet`                       | apps/v1     | always run a pod in all/a set of nodes (for nodes' deamon application, eg. log collector) |
| `StatefulSet`                     | apps/v1     | deploy stateful application                                                               |

## Deployment: Image version Update

- Image `rolling update` Process:

  - Deployment create a new version of replicaSet and create new pod inside it.
  - `rolling update`, while new version pod creation, delete old version pod
  - Deployment terminate the previous version replicaSet.
  - Old versioned replicaSet are kept inside deployment, for `version rollback`
  - when `rollback`, Deployment creating pod inside old versioned replicaSet and delete pod inside new version replicaSet.

- `revision`: replicaSet of different image version

```bash
# two ways to change image version
kubectl set image nginx-deploy nginx-container=nginx:1.16.1
kubectl apply -f nginx-deploy.yml
```

### rolling update

- depolyment specs

  - .spec.`paused`: boolean (deafult false). whether the deploy is paused immediately after deployment is up
  - .spec.`revisionHistoryLimit`: (Clean up Policy) specify how many old versions (ReplicaSets) for this Deployment you want to retain.
  - .spec.`progressDeadlineSeconds`: seconds you want to wait for your Deployment to progress before reports failed
  - .spec.`strategy.type`: strategy used to replace old Pods, when updating image version (kubectl set image)

    - `RollingUpdate`(default): 杀死一部分 pod 创建一部分 pod， 更新过程中会同时存在 2 个版本的 pod
    - `Recreate`: All existing Pods are killed before new ones are created

- .spec.strategy.rollingUpdate
  - `maxUnavailable`: maximum number of Pods that can be unavailable during the update process, default 25% (ie. during updating the workload will at least be 75% of specified value)
  - `maxSurge`: the total number of old and new Pods must not surge (100 + maxSurge)% of the specified value, default 25%

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  labels:
    app: my-app
    type: front-end
spec:
  revisionHistoryLimit: 10
  paused: false
  progressDeadlineSeconds: 600
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 30%
      maxSurge: 30%
  replicas: 3
  selector: # select the target
    matchLabels: # what labels will be affected by this config
      app: my-app
  template: # pod's template (a nested POD config without `apiVersion` and `kind`)
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-app
          image: my-image
          env:
            - name: SOME_ENV
              value: $SOME_ENV
          ports:
            - containerPort: 8080
```

### rollout command

- `kubectl rollout`
  - status, history
  - pause, resume, restart, undo

```bash
# rollout info
k rollout status deploy my-deployment   # status: 查看当前版本升级状态。
k rollout history deploy my-deployment # history: 查看版本升级历史记录 (提示版本号)

# rollout control
k rollout pause # pause: 暂停版本升级过程
k rollout resume # resume: 继续已暂停的版本升级。
k rollout restart # restart: 重启版本升级过程。
k rollout undo deploy my-deployment --to-revision=1 # undo: 回滚到指定版本。
```

### Canary Deployment

- [ref](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments)

- `Canary Deployment` : 也叫灰度发布, 使用 B 版本更新 A 版本。让一部分用户继续用 A，一部分用户开始用 B，如果 B 版本在使用上没有什么大问题，那么逐步扩大范围，把所有用户都迁移到 B 上面来 (progressive rollout of an application)

## Horizontal Pod Autoscaler (HPA)

- resources

  - [bili](https://www.bilibili.com/video/BV1Qv41167ck/?p=53)
  - [docs](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
  - [tutorial:Scaling Kubernetes deployments with Amazon CloudWatch](https://aws.amazon.com/blogs/compute/scaling-kubernetes-deployments-with-amazon-cloudwatch-metrics/)

- Horizontal Pod Autoscaler: Monitoring the usage of pod, and then auto scale Deployment. (eg. maintain 40% cpu)

- metric apis:
  - `metrics.k8s.io`: use [metrics-server](https://github.com/kubernetes-sigs/metrics-server)
  - `custom.metrics.k8s.io`: use your custom metric pipeline
  - `external.metrics.k8s.io`: use external metrics

```yml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: example-hpa
  namespace: dev
spec:
  scaleTargetRef: # target Deployment name
    apiVersion: apps/v1
    kind: Deployment
    name: example-deployment
  minReplicas: 3 # upper/lower bound
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50 # 50% average CPU utilization of the pods
```

```bash
k get hpa -w   # watch hpa related resource's metrics (TARGETS: current%/target%)
```

## DaemonSet

- DaemonSet ensure `every node` (worker node, without taint) has `one copy of DaemonSet pod`
  - 每当有新的 node 添加到集群。DaemonSet 自动部署一份 Pod 到新的 node
  - (side note) the feature is implemented with NodeAffinity
- similar to replicas, but without reps

- Daemon Pod examples:
  - kube-proxy
  - networking components
  - logging
  - monitoring

```yml
# template: simplified
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: example-daemonset
  labels:
    app: example
spec:
  selector:
    matchLabels:
      app: example
  template:
    metadata:
      labels:
        app: example
    spec:
      containers:
        - name: daemon-container
          image: daemon-image:tag
          ports:
            - containerPort: 80
          # Add more container configuration as needed
```

## Job/ CronJob

### Job

- [bili](https://www.bilibili.com/video/BV1Qv41167ck?p=56)
- [docs](https://kubernetes.io/docs/concepts/workloads/controllers/job/)

- Job: a specified number of one-time tasks
  - A Job creates Pods to execute tasks
  - As pods successfully complete and exited, the Job tracks the successful completions
  - When a specified number of successful completions is reached, the task (ie, Job) is complete

```yml
apiVersion: batch/v1
kind: Job
metadata:
  name: job-pod-failure-policy-example
spec:
  completions: 12 # 要求的成功次数
  parallelism: 3 # job在任意时刻并发运行的 pod 数量。
  backoffLimit: 6 # 在 pod 失败后进行重试的次数。
  activeDeadlineSeconds: 30 # 每个task的最大运行时间，超时后将会尝试终止。
  podFailurePolicy: # 略, 通过容器的退出码，不同的处理失败pod
  manualSelector: true # default false, the system automatically generates labels/selector
  selector:
    matchLabels:
      app: job
  template:
    metadata:
      labels:
        app: job
    spec:
      restartPolicy: Never
      containers:
        - name: main
          image: docker.io/library/bash:5
          command: ["bash"] # example command simulating a bug which triggers the FailJob action
          args:
            - -c
            - echo "Hello world!" && sleep 5 && exit 42
```

### CronJob (cj)

- `CronJob` manage `Job`. Create and run job at regular base

```yml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: example-cronjob
spec:
  schedule: "*/5 * * * *" # 开始执行job的时间点 (cron expression)
  concurrencyPolicy: # 前一次job未完成时，是否以及如何运行下一次job: Allow|Forbid|Replace
  successfulJobsHistoryLimit: 3 # 为成功的job保留3个历史记录。
  failedJobsHistoryLimit: 1 # 为失败的job保留3个历史记录。
  startingDeadlineSeconds: # 启动job的超时时长。
  jobTemplate:
    spec:
      completions: 12 # 要求的成功次数
      parallelism: 3 # job在任意时刻并发运行的 pod 数量。
      template:
        metadata:
          name: example-pod
        spec:
          containers:
            - name: example-container
              image: your-image:tag
              command: ["your-command", "arg1", "arg2"]
              # Add more container configuration as needed
          restartPolicy: OnFailure
```

# Data Storage

- [volume](https://kubernetes.io/docs/concepts/storage/volumes/)
- [persistent-volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
- [bili](https://www.bilibili.com/video/BV1Qv41167ck?p=70)

- volume: a mountable storage place
  - `EmptyDir`: temporary container shared storage (host node)
  - `HostPath`: permanent container shared storage (host node)
  - `NFS`: permanent pod shared storage (remote NFS)
- persistent volume: an abstraction to provision volume in k8s
  - `Persistent Volumes(PV)`, `PersistentVolumeClaim (PVC) `

## Volume

- volume:

  - a volume is bound to a pod, requested/mounted by a pod
  - access as a file directory in the file system (think usb stick)
  - volume can be shared by multiple containers in a pod

- volume resources types

  - volume:
    - `EmptyDir`: temporary container shared storage (host node)
    - `HostPath`: permanent container shared storage (host node)
    - `NFS`: permanent pod shared storage (remote NFS)

```yml
pod.spec.volumes
pod.spec.containers.volumeMounts
```

### EmptyDir

- EmptyDir:

  - create an temporary emptyDir in host node,
  - lifecycle bound to a pod (`ephemeral, only for temporary storage.`).

- spec
  - containers.volumeMounts: specify `volume to mount` and `mount path`
  - volumes: declare volumes

```yml
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  containers:
    - name: nginx
      image: nginx:1.17.1
      volumeMounts:
        - name: shared-data
          mountPath: /var/log/nginx
    - name: busybox
      image: busybox:1.30
      volumeMounts:
        - name: shared-data
          mountPath: /logs
  volumes:
    - name: shared-data
      emptyDir: {}
```

### Hostpath

- Hostpath
  - mounts a file or directory from the host node's filesystem into your Pod, `Permanent`
  - `bound to a pod`

> `HostPath volumes present many security risks, and it is a best practice to avoid the use of HostPaths` (https://kubernetes.io/docs/concepts/storage/volumes/#hostpath)

```yml
spec:
  containers:
    - name: nginx
      image: nginx:1.17.1
      volumeMounts:
        - name: shared-data
          mountPath: /var/log/nginx
  volumes:
    - name: shared-data
      hostPath:
        path: /root/logs # path in host
        type: DirectoryOrCreate
        # DirectoryOrCreate: 如果目录不存在就先创建
        # Directory: 目录必须存在
        # File | FileOrCreate
```

### NFS

- NFS
  - allows an path in existing `NFS (Network File System)` to be mounted into a Pod
  - data can be `shared between pods` .
  - You must have your own NFS server running and exported [stepBystep](https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs)

```yml
spec:
  containers:
    - image: registry.k8s.io/test-webserver
      name: test-container
      volumeMounts:
        - mountPath: /my-nfs-data
          name: nfs-volume
  volumes:
    - name: nfs-volume
      nfs:
        server: my-nfs-server.example.com
        path: /my-nfs-volume
        readOnly: true
```

### more

- GlusterFS, iSCSI, Flocker, ceph, AWS, Azure, GCP...

## persistent volume

- [bili](https://www.bilibili.com/video/BV1Qv41167ck?p=77)
- [persistent-volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)

- persistent volume (pv): abstract different volume creation config centrally. for easy reuse by different pods

  - pv is a `cluster-level resource`
  - lifecycle independent of any individual Pod that uses the pv

- persistent volume claim (pvc): a claim from user (pod) to k8s to request persistent volume (pv)

  - k8s binds requested `pvc` to a fitting `pv`, which connect to the underline storage resource. `pvc and pv are one-to-one binding relation`
  - binding rule:
    - `storageClass`: pvc 只能与相同 storageClass 的 pv 匹配. 不指定 storageClass 的 pvc 只能与不指定 storageClass 的 pv 匹配。
    - 可以通过标签 `selector` 进行 filter
    - matching `accessModes`, `volumeMode`
    - pv 可提供空间必须大于 pvc 的要求空间

- pv lifecycle status

  - Available: 可用，等待被 pvc 绑定。
  - Bound: 已经被 pvc 绑定
  - Released: 被绑定的 pvc 已经被删除，但还未可用。
  - Failed: pv's Reclaim failed

- persistentVolumeReclaimPolicy: what happens after a binded pvc is deleted
  - Retain: pv and data will be retained until manually deleted, pv will not be available for reuse
  - Delete: pv will be deleted along with pvc
  - Recycle: data inside volume will be scrubbed (think formatted) before made available for reuse

![](https://imgur.com/vXu5gkA.jpg)

```yml
# pv: I can provide 5Gi nfs volume across the cluster
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity: # pv可以提供的资源。
    storage: 5Gi
  nfs: # the volume resource: hostPath, nfs, csi....
    path: /root/data/pv1
    server: 192.168.109.100
  accessModes:
    - ReadWriteOnce # ReadWriteOnce(RWO): mounted by a single pvc(r/w)|ReadOnlyMany(ROX): mounted by multiple pvc simultaneously(r)|ReadWriteMany(RWX)
  volumeMode: Filesystem
  storageClassName: standard # storageClass: 具有特定类别的pv只能与请求了该类别的 pvc绑定。
  persistentVolumeReclaimPolicy: Retain # 当pv不再被使用对其数据的处理方式。

---
# pvc: this is a request for a 5Gi ReadWriteOnce, standard storageClass volume
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: example-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: standard
  resources:
    requests:
      storage: 5Gi
---
# a pod that use the pvc
spec:
  containers:
    - image: registry.k8s.io/test-webserver
      name: test-container
      volumeMounts:
        - mountPath: /my-pvc-data/
          name: pvc-volume
  volumes:
    - name: pvc-volume
      persistentVolumeClaim:
        claimName: example-pvc
        readOnly: true
```

### Container Storage Interface (CSI)

- Container Storage Interface (CSI):
  - similar to Container Runtime Interface, it is a universal standard Interface to allows any container orchestration tool to work with any storage vendors (Azure, AWS, NetApp...)
  - [ref](https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/17482962)
  - [AWS EBS CSI Driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)

```yml
# prerequisites:
## Kubernetes 1.13+ (CSI 1.0).
## The aws-ebs-csi-driver installed.
## Created an Amazon EBS volume.
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-pv
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 5Gi
  csi:
    driver: ebs.csi.aws.com
    fsType: ext4
    volumeHandle: vol-03c604538dd7d2f41 # string: EBS volume ID:
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: topology.ebs.csi.aws.com/zone
              operator: In
              values:
                - ["us-east-2c"] # availability zone
```

### StorageClass (sc)

- [doc](https://kubernetes.io/docs/concepts/storage/storage-classes/)
- [bili](https://www.bilibili.com/video/BV1MT411x7GH?p=66)

- StorageClass: allow dynamically provision storage and generating pv, when there is a pvc (pod->pvc->sc->pv&volume)

  - pvc initiate a request
  - StorageClass provision the resource in the provisioner(eg. aws-ebs), then generate a pv that binds to pvc

- `VolumeBindingMode`: controls when volume binding and dynamic provisioning should occur.
  - `Immediate`: volume binding and dynamic provisioning occurs once the pvc is created
  - `WaitForFirstConsumer`: delay the binding and provisioning of a pv until a Pod using the pvc is created

```yml
# StorageClass controls pv generation policy
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters: # parameters used to provision aws-ebs (fields are storage provider specific)
  type: gp2
reclaimPolicy: Retain
---
# pvc
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: example-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: standard # ! specify the StorageClass to use
  resources:
    requests:
      storage: 5Gi
```

## Parameter Storage:

- configMap (cm): store env var
  - create configMap (define env variables in key-value pair)
  - inject configMap into pod

```bash
# create configMap kubectl way
k create cm app-config \
  --from-literal=APP_MODE=prod \
  --from-literal=APP_VALUE=red \
k create cm app-config --from-file=/path/to/.env
```

```yml
# create configMap config file way
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  APP_MODE: prod
  APP_VALUE: red
```

- secret: store sensitive information

  - create secret (define env variables in key-value pair)
  - inject secret into pod

- secret safety

  - secret itself is not safe: not encrypted, only encoded (easy to decode)
  - Any pod in the same namespace can access the secret: consider configure `least-privilege access control, Role-based access control (RBAC)`
  - secret is not encrypted `in etcd`: consider `Encryption at Rest`

- secret best practice:
  - Not checking-in secret config files to source code repositories.
  - Enabling `Encryption at Rest` for Secrets in ETCD [ref](https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)
  - Recommended to use `third party secret stores`: Hashicorp Vault, AWS Secrets Manager... (CKS cert)

```bash

# use base64 to encode/decode (not-safe)
echo -n "123321" | base64
echo -n "MTIzMzIx" | base64 -d

# create secret kubectl way
k create secret generic app-secret \
  --from-literal=API_KEY=MTIzMzIx \
  --from-literal=APP_VALUE=UIhMeixa \
k create secret generic app-secret --from-file=/path/to/.env
```

```yml
# create secret config file way
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
data:
  DB_PASSWORD: echo -n "123321" | base64
  API_KEY: echo -n "api_key_123321" | base64
```

### inject configMap/secret to container

```yml
# pod definition
spec:
  containers:
    - image: registry.k8s.io/test-webserver
      name: test-container
      ports:
        - containerPort: 8080
      envFrom: # inject whole configMap as env var
        - configMapRef:
          name: my-configmap
        - secretRef:
          name: my-secret
      env:
        - name: APP_VALUE # inject single env var
          valueFrom:
            configMapKeyRef:
              name: my-configmap
              key: APP_VALUE
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: my-secret
              key: API_KEY
---
# can also mount configMap to pod as volume (file that store config key-value)
spec:
  containers:
    - image: registry.k8s.io/test-webserver
      name: test-container
      ports:
        - containerPort: 8080
      volumeMounts:
        - name: config-volume
          mountPath: /configmap/config
        - name: secret-volume
          mountPath: /secret
  volumes:
    - name: config-volume # inject configMap
      configMap:
        name: my-configmap
    - name: secret-volume # inject secret
      configMap:
        name: my-secret
```
