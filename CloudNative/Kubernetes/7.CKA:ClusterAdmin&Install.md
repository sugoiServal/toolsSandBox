```bash
/etc/kubernetes/
```

# Kube Components

- files
  - `--kubeconfig`: The path to the `kubeconfig file` that the component uses to communicate with the Kubernetes control plane (authentication, etc..).

## Scheduler

- files
  - `--config`: Specifies the path to the kubelet configuration file.

### Deploy Scheduler

- [Configure Multiple Schedulers](https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)

- you can deploy additional kube-scheduler to the default-scheduler (setup by kubeadm).
  - with your custom scheduling logic, and instruct specify pod to use the scheduler.

```yml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
    - command:
        - /usr/local/bin/kube-scheduler
        - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml # setting custom scheduler config
      image: registry.k8s.io/kube-scheduler:v1.27.0
      name: kube-scheduler
      volumeMounts:
        - name: config-volume
          mountPath: /etc/kubernetes/my-scheduler
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-scheduler-config
  namespace: kube-system
data: # schedulerName: my-scheduler is the name to use the scheduler
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1beta2
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler   
    leaderElection:
      leaderElect: false
---
# role, rolebinding, service account...
```

- configure a pod to use the scheduler

```yml
apiVersion: v1
kind: Pod
metadata:
  name: us-my-scheduler
spec:
  schedulerName: my-scheduler # # schedulerName: my-scheduler the scheduler to use
  containers:
    - name: pod-with-default-annotation-container
      image: registry.k8s.io/pause:2.0
```

### Extend and Customize Schedulers

- [Scheduler Configuration](https://kubernetes.io/docs/reference/scheduling/config/)
- [Extension Point](https://kubernetes.io/docs/reference/scheduling/config/#extension-points)

- `extension point`: Schedulers use plugins name `extension point` to customize the behavior of a Scheduler

  - `extension point` are mount in `KubeSchedulerConfiguration: - plugins`

- `profiles`: with profiles you don't need to deploy multiple schedulers to use different extension point plugin sets
  - a `KubeSchedulerConfiguration` can have multiple profiles (a `schedulerName`), and define a set of plugins within it.
  - `KubeSchedulerConfiguration` is mount to a `Scheduler process`, who scheduler differently using different profile based on the pod's need

```yml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
    plugins:
      score:
        disabled:
          - name: PodTopologySpread
        enabled:
          - name: MyCustomPluginA
            weight: 2
          - name: MyCustomPluginB
            weight: 1
  - schedulerName: my-scheduler-2
    plugins:
      score:
        disabled:
          - name: TaintToleration
        enabled:
          - name: "*"
```

## Kubelet

- files
  - `--config`: Specifies the path to the kubelet configuration file.
  - `--bootstrap-kubeconfig`: The kubeconfig file used by the kubelet for cluster bootstrap. This is used during the node registration process.

```bash
ps -aux | grep kubelet
```

### Static Pods

- [doc](https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/)
- [udemy](https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/14863335#overview)

- Static Pods: Static Pods are managed directly by the `kubelet`

  - `API server is not involved` in the management lifecycle of static Pods.
  - use case:
    - `manually deploy controlplane components` (etcd/apiserver/controllermanager) in master nodes
    - Run node Initialization/ Bootstrapping logic
    - Use in environment where no API server(eg, cannot connect to master node) available

- How:

  - place pod definition in `/etc/kubernetes/manifests/kube-apiserver.yaml`
  - Configure kubelet on the node to use this directory: `--pod-manifest-path`
  - kubelet automatically start the pod and manage the pod's lifecycle (create/restart/remove if file deleted)

- notes
  - static pod has the node name in the end of its name (eg; controlplane: kube-apiserver-controlplane)

```bash
# find out the static pod manifest directory
ps -aux | grep kubelet   # --config=/var/lib/kubelet/config.yaml
cat /var/lib/kubelet/config.yaml
# staticPodPath: /etc/kubernetes/manifests
```

# Monitoring: `Metrics Server`

- what to monitor:

  - node-level metrics: number of healthy nodes, performance metrics: CPU, Mem, Network...
  - pod level metrics: number of healthy pods, performance metrics: CPU, Mem, Network...

- kubernetes Doesn't have a built-in monitoring solution right now. Open-source solutions

  - `Metrics Server`, `Prometheus`, the `Elastic Stack`

- `Metrics Server` is a `in-memroy` kubernetes monitoring project, that collects pods and nodes metrics from `Kubelets`. Through a subcomponent cAdvisor or Container Advisor.
  - does not store the metrics on the disk. So cannot see historical performance data.

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# use metric server
kubectl top node
kubectl top pod
```

# Cluster Upgrade

### drain/ cordon

- drain node

  - When you drain a node, the pods are gracefully terminated from the node, and rescheduled on another node if it belongs to a replicaSet.
  - if there are standalone pods that not belongs to a replicaSet, it will be lost
  - the node is also `cordon`: marked as unschedulable
  - `usage`: you need to conduct maintainance to the node, reboot the node

- after drain (and node patchk):

  - you need to manually `uncordon`: unmarked the unschedulable status
  - the pods that has been rescheduled will not come back

```bash
k drain node01 --ignore-deamonsets
k uncordon node01
k cordon node01   # mark a node unschedulable
```

## Kubernete Cluster Upgrade

- facts:

  - `etcd, CoreDNS, CNI providers` are separate project from kubernetes, So they have their own versioning
  - Kubernetes supports only up to the 3 most recent minor versions.
  - The Kubernetes components in a cluster don't need to be at a same version.
    - kube-apiserver must have the highest vision.
    - Other components can have one to two minor version lower than the kube-apiserver.

- kuberenets recommends to `upgrade one minor version at a time`.

- version upgrade
  - If you use a managed Kubernetes provider: EKS, Google Kubernetes Engine, The version update is as simple as a few clicks.
  - If you maintain the cluster by yourself, you can use `kubeadm` to upgrade

### Upgrade with kubeadm

- [kubeadm-upgrade](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
- [Upgrading Linux worker nodes](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/)
- [upgrade demo video](https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/24458188)

- step one: upgrade master nodes

  - when master node goes offline, work nodes can still work independently
  - You cannot do administration task or use the `kubectl`

- step two: upgrade worker nodes

  - `strategies-1`: rolling upgrade(no downtime): upgrade `one node at a time`, follow the `drain->uncordon` procedure
  - `strategies-2`: upgrade by adding nodes with new version, and then migrate the workload (no downtime): Can be easily done in cloud providers environments

- `kubeadm` can be used to upgrade `master node components`

  - kubeadm also follow the same version as kubernetes

- verifications

```bash
# get kubernetes version
k version --short

# get the latest kubernete/kubeadm version
apt-cache madison kubeadm

# get latest version current kubeadm can install
kubeadm upgrade plan

# ubuntun release
cat /etc/*release*
```

- upgrade master node

```bash
# step 1: Upgrade kubeadm
apt-get upgrade -y kubeadm=1.13.0-00
# step 2: see the upgrade plan
kubeadm upgrade plan
# step 3: Apply Upgrade
kubeadm upgrade apply v1.13.4
```

- upgrade kubelets in nodes

```bash
# step 1: drain node from kube-api (make sure there is no pod not contolled by pod controller)
k drain node01
# step 2: upgrade kubeadm and node configuration
apt-get upgrade -y kubeadm=1.13.0-00
kubeadm upgrade node config --kubelet-version v1.13.0  #
# step 3: upgrade kubelet
apt-get upgrade -y kubelet=1.13.0-00
# step 4: restart kubelet
systemctl restart kubelet
systemctl daemon-reload
# step 5: uncordon node from kube-api
k uncordon node01
```

# ETCD Backup/Restore

# Cluster Troubleshoot

# Cluster Setup
