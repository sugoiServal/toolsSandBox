```bash
/etc/kubernetes/
```

# Kube Components

- files
  - `--kubeconfig`: The path to the `kubeconfig file` that the component uses to communicate with the Kubernetes control plane (authentication, etc..).

## Scheduler

- files
  - `--config`: Specifies the path to the kubelet configuration file.

### Deploy Scheduler

- [Configure Multiple Schedulers](https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)

- you can deploy additional kube-scheduler to the default-scheduler (setup by kubeadm).
  - with your custom scheduling logic, and instruct specify pod to use the scheduler.

```yml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
    - command:
        - /usr/local/bin/kube-scheduler
        - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml # setting custom scheduler config
      image: registry.k8s.io/kube-scheduler:v1.27.0
      name: kube-scheduler
      volumeMounts:
        - name: config-volume
          mountPath: /etc/kubernetes/my-scheduler
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-scheduler-config
  namespace: kube-system
data: # schedulerName: my-scheduler is the name to use the scheduler
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1beta2
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler   
    leaderElection:
      leaderElect: false
---
# role, rolebinding, service account...
```

- configure a pod to use the scheduler

```yml
apiVersion: v1
kind: Pod
metadata:
  name: us-my-scheduler
spec:
  schedulerName: my-scheduler # # schedulerName: my-scheduler the scheduler to use
  containers:
    - name: pod-with-default-annotation-container
      image: registry.k8s.io/pause:2.0
```

### Extend and Customize Schedulers

- [Scheduler Configuration](https://kubernetes.io/docs/reference/scheduling/config/)
- [Extension Point](https://kubernetes.io/docs/reference/scheduling/config/#extension-points)

- `extension point`: Schedulers use plugins name `extension point` to customize the behavior of a Scheduler

  - `extension point` are mount in `KubeSchedulerConfiguration: - plugins`

- `profiles`: with profiles you don't need to deploy multiple schedulers to use different extension point plugin sets
  - a `KubeSchedulerConfiguration` can have multiple profiles (a `schedulerName`), and define a set of plugins within it.
  - `KubeSchedulerConfiguration` is mount to a `Scheduler process`, who scheduler differently using different profile based on the pod's need

```yml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
    plugins:
      score:
        disabled:
          - name: PodTopologySpread
        enabled:
          - name: MyCustomPluginA
            weight: 2
          - name: MyCustomPluginB
            weight: 1
  - schedulerName: my-scheduler-2
    plugins:
      score:
        disabled:
          - name: TaintToleration
        enabled:
          - name: "*"
```

## Kubelet

- files
  - `--config`: Specifies the path to the kubelet configuration file.
  - `--bootstrap-kubeconfig`: The kubeconfig file used by the kubelet for cluster bootstrap. This is used during the node registration process.

```bash
ps -aux | grep kubelet
```

### Static Pods

- [doc](https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/)
- [udemy](https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/14863335#overview)

- Static Pods: Static Pods are managed directly by the `kubelet`

  - `API server is not involved` in the management lifecycle of static Pods.
  - use case:
    - `manually deploy controlplane components` (etcd/apiserver/controllermanager) in master nodes
    - Run node Initialization/ Bootstrapping logic
    - Use in environment where no API server(eg, cannot connect to master node) available

- How:

  - place pod definition in `/etc/kubernetes/manifests/kube-apiserver.yaml`
  - Configure kubelet on the node to use this directory: `--pod-manifest-path`
  - kubelet automatically start the pod and manage the pod's lifecycle (create/restart/remove if file deleted)

- notes
  - static pod has the node name in the end of its name (eg; controlplane: kube-apiserver-controlplane)

```bash
# find out the static pod manifest directory
ps -aux | grep kubelet   # --config=/var/lib/kubelet/config.yaml
cat /var/lib/kubelet/config.yaml
# staticPodPath: /etc/kubernetes/manifests
```

# Monitoring: `Metrics Server`

- what to monitor:

  - node-level metrics: number of healthy nodes, performance metrics: CPU, Mem, Network...
  - pod level metrics: number of healthy pods, performance metrics: CPU, Mem, Network...

- kubernetes Doesn't have a built-in monitoring solution right now. Open-source solutions

  - `Metrics Server`, `Prometheus`, the `Elastic Stack`

- `Metrics Server` is a `in-memroy` kubernetes monitoring project, that collects pods and nodes metrics from `Kubelets`. Through a subcomponent cAdvisor or Container Advisor.
  - does not store the metrics on the disk. So cannot see historical performance data.

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# use metric server
kubectl top node
kubectl top pod
```

# Cluster Upgrade

### drain/ cordon

- drain node

  - When you drain a node, the pods are gracefully terminated from the node, and rescheduled on another node if it belongs to a replicaSet.
  - if there are standalone pods that not belongs to a replicaSet, it will be lost
  - the node is also `cordon`: marked as unschedulable
  - `usage`: you need to conduct maintainance to the node, reboot the node

- after drain (and node patchk):

  - you need to manually `uncordon`: unmarked the unschedulable status
  - the pods that has been rescheduled will not come back

```bash
k drain node01 --ignore-deamonsets
k uncordon node01
k cordon node01   # mark a node unschedulable
```

## Kubernete Cluster Upgrade

- facts:

  - `etcd, CoreDNS, CNI providers` are separate project from kubernetes, So they have their own versioning
  - Kubernetes supports only up to the 3 most recent minor versions.
  - The Kubernetes components in a cluster don't need to be at a same version.
    - kube-apiserver must have the highest vision.
    - Other components can have one to two minor version lower than the kube-apiserver.

- kuberenets recommends to `upgrade one minor version at a time`.

- version upgrade
  - If you use a managed Kubernetes provider: EKS, Google Kubernetes Engine, The version update is as simple as a few clicks.
  - If you maintain the cluster by yourself, you can use `kubeadm` to upgrade

### Upgrade with kubeadm

- [kubeadm-upgrade](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
- [Upgrading Linux worker nodes](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/)
- [upgrade demo video](https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/24458188)

- step one: upgrade master nodes

  - when master node goes offline, work nodes can still work independently
  - You cannot do administration task or use the `kubectl`

- step two: upgrade worker nodes

  - `strategies-1`: rolling upgrade(no downtime): upgrade `one node at a time`, follow the `drain->uncordon` procedure
  - `strategies-2`: upgrade by adding nodes with new version, and then migrate the workload (no downtime): Can be easily done in cloud providers environments

- `kubeadm` can be used to upgrade `master node components`

  - kubeadm also follow the same version as kubernetes

- verifications

```bash
# get kubernetes version
k version --short

# get the latest kubernete/kubeadm version
apt-cache madison kubeadm

# get latest version current kubeadm can install
kubeadm upgrade plan

# ubuntun release
cat /etc/*release*
```

- upgrade master node

```bash
# step 1: Upgrade kubeadm
apt-get upgrade -y kubeadm=1.13.0-00
# step 2: see the upgrade plan
kubeadm upgrade plan
# step 3: Apply Upgrade
kubeadm upgrade apply v1.13.4
```

- upgrade kubelets in nodes

```bash
# step 1: drain node from kube-api (make sure there is no pod not contolled by pod controller)
k drain node01
# step 2: upgrade kubeadm and node configuration
apt-get upgrade -y kubeadm=1.13.0-00
kubeadm upgrade node config --kubelet-version v1.13.0  #
# step 3: upgrade kubelet
apt-get upgrade -y kubelet=1.13.0-00
# step 4: restart kubelet
systemctl restart kubelet
systemctl daemon-reload
# step 5: uncordon node from kube-api
k uncordon node01
```

# Backup/Restore

- Resource config backup solutions

  - use github (store IaaC configs)
  - use velero (snapshot of current cluster as yaml, use apiserver)

- etcd backup solution: etcd has built-in backup solution

## etcd Backup/Restore

- [Backing up an etcd cluster](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster)

- `etcd.service: --data-dir`: specify where etcd data are located

```bash
# for TLS-enabled etcd, provide the required cert/key
etcdctl snapshot save /path/to/snapshot.db  \
      --endpoints https://10.2.0.9:2379 \
      --cacert=/etc/etcd/ca.crt \
      --cert=/etc/etcd/etcd-server.crt \
      --key=/etc/etcd/etcd-server.key
```

```bash
# etcd backup
ETCDCTL_API=3 etcdctl snapshot save /path/to/snapshot.db   # take a snapshot
ETCDCTL_API=3 etcdctl snapshot status /path/to/snapshot.db  # view snapshot details

# etcd restore
# stop kube apiserver because it depends on etcd
service kube-apiserver stop
# restore from backup
ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 snapshot restore snapshot.db --data-dir /path/to/etcd/data-dir-from-backup
# reconfig etcd to use the new data directory
etcd.service: --data-dir=/path/to/etcd/data-dir-from-backup
# restart etcd and apiserver
service etcd restart
service kube-apiserver start
```

# Cluster Troubleshoot

## Application Failure

- [Troubleshooting Applications](https://kubernetes.io/docs/tasks/debug/debug-application/)
- checklist
  - service
  - network policy
  - application pod (logs, status, lifecycle)

### POD status

- `CrashLoopBackOff`: CrashLoop: pod repeatedly crashes immediately upon starting, BackOff: attempting to restart after crash
- `ImagePullBackOff`: image cannot be retrieved from the container registry.

- Pod phase

  - `Pending`: pod 成功被创建，但是可能 pod 还没有被 schedule， 或者有容器还没有被 set up。
  - `Running`: 所有容器已经创建完成。
  - `Succeeded`: 所有容器成功退出。(eg, a batch job)
  - `Failed`: 所有容器已经退出,至少有一个不成功。
  - `Unknown`: For some reason the state of the Pod could not be obtained (eg, network issue)

- Container states

  - `Waiting`: neither Terminated or Running: for example, pulling the container image, or applying Secret data
  - `Running`: executing without issues.
  - `Terminated`: Either ran to completion or failed for some reason.

### Pod Controller

- DESIRED: # replicaset specified
- CURRENT: # pod currently runing
- READY: # service ready
- UP-TO-DATE: # pod in the latest version

## controlplane Failure

- [Troubleshooting Clusters](https://kubernetes.io/docs/tasks/debug/debug-cluster/)
- [Troubleshooting kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/)

## worker node failures

## Network Troubleshooting

# Cluster Setup

- [bili](https://www.bilibili.com/video/BV1Qv41167ck?p=7)
- [udemy](https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/20666298)

- [install-containerd-1](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)
- [install-containerd-2](https://github.com/containerd/containerd/blob/main/docs/getting-started.md#option-2-from-apt-get-or-dnf)

- [install kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)
- [Creating a cluster with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)

### cgroup drivers

- On Linux, `control groups` are used to constrain resources that are allocated to processes.

- setup nodes, network nodes, dns...
- install container runtime in each of the nodes
- install kubeadm, kubectl, kubelet in each of the nodes
- kubeadm init in master node
  - install apiserver, scheduler, controller-manager, etcd in controlplane,
  - automatically TLS Certificate Generation
  - kubeconfig generation: generate a default kubeconfig for kubectl in .kube
  - bootstrap token: generates a bootstrap token that allows a node to join the cluster
- follow kubeadm instruction to join nodes to the cluster
- follow kubeadm instruction to setup a network component (CNI: weave, flannel...)

```bash
kubeadm init
```

### install k8s cluster the HARDCORE way

- [install k8s cluster the HARDCORE way](https://www.youtube.com/watch?v=uUupRagM7m0&list=PL2We04F3Y_41jYdadX55fdJplDvgNGENo)
