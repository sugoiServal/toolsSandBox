### tldr

```bash
# ssh
ssh node01

# interface info
ip addr
ip addr show type bridge # filter by type: bridge

# router/gateway info
route
ip route

# port info
netstat -npl | grep -i kube-scheduler  # what port a process is listening to
netstat -npa | grep -i etcd | grep 2379 | wc -l # how many sockets a port is connected


# cni directories
--cni-bin-dir=/opt/cni/bin    # dir of cni plugins as executable
--cni-conf-dir=/etc/cni/net.d # configuration where kubelet find cni to use
```

- ip range problems

```bash
# node ip ranges
ip addr # from node/controlplane

# pod ip ranges
k logs weave-net-lgmtf -n kube-system

# service ip range
k get pod kube-apiserver-controlplane -n kube-system -o yaml   # --service-cluster-ip-range=10.96.0.0/12

# what type of proxy the kube-proxy use
k logs kube-proxy-w59bh -n kube-system
```

- use cni
  - prerequsite:
    - base Kubernetes system in cluster is ready
    - nodes networking configured

```bash
# weave will be deployed as deamonset in worker nodes, in kube-system namespace
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

# check weave is up
kubectl get pods -n kube-system | grep -i "weave"

# then config the node to use weave in 10-weave.conflist
cat /etc/cni/net.d/10-weave.conflist
```

- dns

```bash
cat /etc/hosts
cat /etc/resolv.conf
```

# Linux Networking, CNI

## Basic: Switch/Routing

- `Interface`: a host need an interface to communicate

```bash
# show the interfaces (eth0)
ip link  # 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
```

- hosts in a LAN communicates with a `switch`. In abstraction, `a switch is a table in the host storing other hosts in the LAN`

```bash
# add another host's LAN ip (192.168.1.10/24) to this host
ip addr add 192.168.1.10/24 dev eth0
```

- hosts in separate LAN networks communicates with a `router`. router is a host machine that can talk to both network
  - `gateway`: the doorway to/out a network. In context of router, `a gateway of a LAN` is `the local ip address of a router`

```bash
# see the existing routing configuration (routing table) on a host
route

# add a gateway: router (192.168.1.1) to another network (192.168.2.0/24)
    # to go to network 192.168.2.0/24, you have to go through the host 192.168.1.1 in your network
ip route add 192.168.2.0/24 via 192.168.1.1

# add a gateway: router (192.168.1.1) as default to any network
# default is the same as 0.0.0.0 : all ip address
ip route add default via 192.168.1.1
```

- setup a linux host as a router

```bash
# assume 192.168.1.1 and 192.168.2.1 refers to the same router host in two networks

# add this to all hosts in network 192.168.1.0/24
ip route add 192.168.2.0/24 via 192.168.1.1
# add this to all hosts in network 192.168.2.0/24
ip route add 192.168.1.0/24 via 192.168.2.1

# In router's host: config to allow ip forward
echo 1 > /proc/sys/net/ipv4/ip_forward
# do this to persist the setting after reboot
    # /etc/sysctl.conf
    net.ipv4.ip_forward = 1

# router working
ping 193.168.2.5 # from 192.168.1.5
```

## Network Namespace in Linux

- [ref](https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/14296142)
- Containers use Linux Network namespaces to implement network isolation.

  - Containers can see net resources in their namespace, but cannot see net resources in other namespace or the host

- think namespace a host within a host, which also represent a private network

```bash
# create namespaces
ip netns add red # create a Network Namespace
ip netns # list all Network Namespace

# exec command in a namespaces
ip netns exec red ip link # exec a command: ip link inside "red" namespace
ip netns exec red route # exec a command: route inside "red" namespace
ip -n red link  # run ip link inside namespace "red"
```

- connect two namespace directly
  - veth: virtual Ethernet devices

```bash
# create a pipe (virtual connection) between two ns
ip link add veth-red type veth peer name veth-blue
# connect virtual devices (veth) to both end
ip link set veth-red netns red
ip link set veth-blue netns blue
# assigned ips to namespaces,
ip -n red addr add 192.168.15.1 dev veth-red
ip -n blue addr add 192.168.15.2 dev veth-blue
# bring up the interfaces
ip -n red link set veth-red up
ip -n blue link set veth-blue up
# test connection from red
ip netns exec red ping 192.168.15.2
# delete veth
ip -n red link del veth-red # the other end is deleted automatically
```

- connect multiple namespace with a `linux bridge` (virtual switch)
  - for the host, it is a net device,
  - for a namespaces, it is like a swtich that it can connect to

```bash
# create a bridge and bring it up
ip link add v-net-0 type bridge
ip link set dev v-net-0 up

#  create a pipe (virtual connection) between ns red and bridge device
ip link add veth-red type veth peer name veth-red-bridge
# connect virtual devices (veth) to both end
ip link set veth-red netns red
ip link set veth-red-bridge master v-net-0
# assigned ips to namespaces red
ip -n red addr add 192.168.15.1 dev veth-red
# bring up the interfaces
ip -n red link set veth-red up

# do the same thing to another ns blue
ip link add veth-blue type veth peer name veth-blue-bridge
ip link set veth-blue netns blue
ip link set veth-blue-bridge master v-net-0
ip -n blue addr add 192.168.15.2 dev veth-blue
ip -n blue link set veth-blue up

# test connection from red
ip netns exec red ping 192.168.15.2
```

- connect host to namespaces use bridge
  - bridge (virtual switch) is actually a network interface for the host.
  - host can connect to the switch by directly assign an ip to it

```bash
# assign an ip, add bridge device as an interface in host
ip addr add 192.168.15.6/24 dev v-net-0
# test: ping red namespace from localhost
ping 192.168.15.1
```

- (outbound) connect a namespace to the extrnal-host in LAN
  - `localhost is the gateway` that connect namespace networks and the external network

```bash
# assume localhost has two ips
  # in bridge network: 192.168.15.5
  # in LAN network: 192.168.1.2
# assume a extrnal-host in LAN is 192.168.1.3

# we want to connect blue namespace in bridge network 192.168.15.2 to extrnal-host in LAN: 192.168.1.3

# add localhost as a gateway, in blue's bridge network, to LAN network 192.168.1.0
ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5

#  add NAT functionality (NAT = Network Address Translation) to the gateway(localhost)
iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE
```

- (outbound) connect a namespace to the Internet
  - still, `localhost is the gateway`

```bash
# use localhost as default gateway of blue ns
ip netns exec blue ip route add default via 192.168.15.5
# test:
ip netns exec blue ping 8.8.8.8
```

- (Inbound) use `port forwarding` to allow inbound traffic from LAN or the Internet to ns

```bash
# forward any traffic reaching localhost port 80 to the blue ns at port 80
iptables -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2:80 -j DNAT
```

- Docker Bridge Network
  - Bridge Network is implemented with namespace,
    - a bridge network in docker is a `linux bridge`, act as a virtual switch for containers in the bridge network
    - each container in bridge network is essentially a namespace, has a ip assigned in the virtual network
    - docker create virtual cable(pipeline) to enable communication between containers in the network
    - Containers are published to external newtorks through port mapping

```bash
# get inforamtion about docker default bridge network
ip link
ip addr

# port mapping
docker run -p 8080:80 nginx
iptables -nvL -t nat  # see port mapping's nat rule created by docker
```

## DNS, Core DNS

- file `/etc/hosts` store all the hosts' name and ip of a linux system

  - think as a `local DNS known only to yourself` (eg. localhost, name used by k8s...)

- `name resolution`: the act of translatinge a host name to a ip address

```bash
# add a host's domain name
echo "192.168.1.11 db" >> /etc/hosts
ping db   # host name db is resolved
```

- DNS server: `centrally store` host names, and accept `name resolution queries`
  - hosts query the DNS server for a name resolution
  - `/etc/resolv.conf`: the file that store the DNS server location
  - `/etc/nsswitch.conf`: set the preference of using: local /etc/hosts file or DNS server for a same entry

```bash
cat /etc/resolv.conf
cat /etc/nsswitch.conf   # host:  files dns (files preferred)
```

- domain name hierarchy

  - domain name has a `inverted hierarchy`, eg. drive.google.com
  - .com is `top level domain name`, a root DNS will forward it to .com DNS
  - .com DNS will forward to google's orgranizational DNS
  - google.com together are call `domain apex`, ie managed by public DNS instead organization DNS
  - google's orgranizational DNS then resolve the `subdomain`: drive

- resolve a private domain name by its subdomain name
  - say you want to use `web` to access `web.mycompany.com` within you company's network

```bash
echo "search mycompany.com" >> /etc/resolv.conf
# then when there are unknown records like: web, host will try to append a mycompany.com before send to DNS
```

- record types:

  - A: ipv4
  - AAAA: ipv6
  - CNAME: map a record name to another name

- DNS troubleshooting tools

```bash
ping
nslookup www.google.com
dig www.google.com
```

### takeaway

```bash
# files
/etc/hosts  # save local host resolution
/etc/resolv.conf  # save DNS server
/etc/nsswitch.conf  # use /etc/hosts or DNS server

# tools
ping
nslookup www.google.com
dig www.google.com
```

# Kubernetes Pod networking

- List of Process Ports in k8s
  - [List of Ports to be used by k8s](https://kubernetes.io/docs/reference/networking/ports-and-protocols/)

## Pod Networking: CNI (container network interface)

- [overview](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model)
- [install cni addons](https://kubernetes.io/docs/concepts/cluster-administration/addons/)

- pod networking follows the same pattern (namespace, bridge switch) as container in host node

- CNI: CNI defines how the plugins should be developed(a set of apis to networking containers), and how container run times should invoke them.

  - plugins: programs that setup container's network (eg, a bridge network)
  - apis: ADD, DEL...

- CNI should follow the following pod networking requirements

  - Every POD should have an IP Address
  - Using the IP address, every POD should be able to communicate with other PODs in the same node.
  - Using the IP address, Every POD should be able to communicate with other PODs on other nodes without NAT.

- plugins examples

  - built-in: Bridge, VLAN, IP VLAN, MAC VLAN, Windows Host-Local and DHCP.
  - third-party: Weave, Flannel, Cilium, VMware NSX, ...

- CNI consuming process:

  - pod is created by kubelet
  - Whenever a container is created, the kubelet looks at the CNI configuration and identify the plugin script
  - kubelet exectue the plugin's ADD command to create the networking for the container

- show kubelet cni configuration directories

```bash
ps -aux | grep kubelet
# kubelet.service
--cni-bin-dir=/opt/cni/bin    # dir of cni plugins as executable
--cni-conf-dir=/etc/cni/net.d # configuration where kubelet find cni to use
--network-plugin=cni
```

### Pod IP address management

- CNI plugin is responsible for

  - assign non-duplicated pod IPs

- `DHCP` and `host-local` are plugins to make sure non-duplicated IPs are drawed

  - pod networking plugins like `WeaveWork will invoke these plugin` to assign non-duplicated pod IPs

- `ipam`: is a section in cni conf file to specify the plugin to used to manage IP address: plugin to use and pod subnet range

```bash
#  /etc/cni/net.d/*.conf
"ipam": {
  "type": "host-local",
  "subnet": "10.244.0.0/16"   # pod subnet's ip range
  "routes": [
    { "dst": "0.0.0.0/0" }
  ]
}
```

### In action: consume WeaveWorks CNI plugins

- prerequsite:
  - base Kubernetes system in cluster is ready
  - nodes networking configured

```bash
# weave will be deployed as deamonset in worker nodes, in kube-system namespace
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

kubectl get pods -n kube-system | grep -i "weave"

# then config the node to use weave in 10-weave.conflist
cat /etc/cni/net.d/10-weave.conflist

# {
#     "cniVersion": "0.3.0",
#     "name": "weave",
#     "plugins": [
#         {
#             "name": "weave",
#             "type": "weave-net",
#             "hairpinMode": true
#         },
#         {
#             "type": "portmap",
#             "capabilities": {"portMappings": true},
#             "snat": true
#         }
#     ]
# }
```

# Service Networking, Core DNS, Ingress

- In developing/production, you would rarely configure your pods to communicate directly with each other. (that is done through CNI setup when setting the k8 cluster). Instead, communication are abstracted through `abstracted networking resources`.

  - `Service`: (layer 4, TCP/UDP) service discovery & load balancer .
    - `ClusterIP`(default): create an permant in-cluster serviceIP
    - `NodePort`: expose a port from any k8s node to a ClusterIP port
    - `LoadBalancer`: build upon NodePort,使用外部云环境的负载均衡器来完成 Request 到 Node:NodePort 的负载均衡。
    - `ExternalName：` 把集群外部的 service 引入到集群，可以从集群内部访问
  - `coredns`
  - `Ingress`: layer 7 (HTTP) reverse-proxy, load balance (expose ClusterIP service to outside as domainNames)
    - provide a list of mapping: domainName => service(ClusterIP). route HTTP request based on the mapping.

# Service proxy mode

- service is abstract resource. kube-proxy maintain a map from ClusterIP to the actual podIP:port of a service, and the map is propagated throughout all nodes in the cluster. There are different proxy mode to create theses maps

  - userspace
  - iptables(default)
  - ipvs

- cluster ip range should not be overlap with pods' ip range

```bash
# set services' proxy mode
kube-proxy --proxy-mode [userspace | iptables | ipvs]

# set services' cluster ip range
kube-api-server --service-cluster-ip-range 10.0.0.0/24

# see rule created by kube-proxy
iptables -L -t nat
```

## CoreDNS

- kubeadm setup a built-in DNS server by default, Otherwise you setup the DNS server using `CoreDNS` by yourself

- `CoreDNS` is a program to run in a DNS servier. In kubernetes, it is `deployed as a pod in kube-system ns`
  - CoreDNS add DNS record each time a service/pod is created

```bash
# sample dns name for service
http://<service-name>.<namespace>.svc.cluster.local:<service-port>
# sample dns name for pod
http://10-244-2-5.<namespace>.pod.cluster.local:<application-port>
```

- Corefile is where CoreDNS configuration are set. Corefile is mounted as a configMap

```bash
# location in pod
cat /etc/coredns/Corefile
```

- kubelet store the CoreDNS's ip address

```bash
cat /var/lib/kubelet/config.yaml
clusterDNS: [10.96.0.10]
```

### Setup CoreDNS manually

- [ref](https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/14410330)
- download, extract, run executable
- DNS server by default listens on `port 53`
- provide IP to hostname mappings
  - eg. save mapping to `/etc/hosts`, then config Core DNS to use the file

```bash
cat > Corefile
. {
    host /etc/hosts
}
```

## Ingress (ing)

- [bili](https://www.bilibili.com/video/BV1Qv41167ck?p=68)
  = [doc](https://kubernetes.io/docs/concepts/services-networking/ingress/)
- Ingress solve the following problems

  - route HTTP traffic to different services based on the url
  - tls cert for a website (https proxy)

- facts

  - Ingress also need to be exposed to outside k8s cluster, like a NodePort or LoadBalancer
  - Ingress is abstraction to supported solutions: nginx, haproxy, traefik... simplify the usage to configuration

- Ingress = Ingress Controller + Ingress Resource

  - Ingress Resource is managed by `Ingress Controller`.
    - you have to deploy a `Ingress Controller` manually.
    - `Ingress Controller` monitor Ingress Resources and automatically configure the solutions (eg.nginx)

- default-http-backend: a ingress resource automatically deploy a default-http-backend rule for non-matching request, you much create such a service (eg, 404 page)

### Deploy `Ingress Controller`

- Ingress Controller detect Ingress Resource, then generate and execute a Nginx configuration (or other solution)

- `Ingress Controller` should be created in its own namespace

- [Ingress-Nginx Controller](https://kubernetes.github.io/ingress-nginx/deploy/#quick-start)

- deploy includes
  - Deployment
  - ns: ingress-nginx
  - Service
  - ServiceAccount, Role, RoleBinding
  - ConfigMap

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml
```

### Configure DNS

- configure DNS to point all subdomains of your website to to the service to Ingress Controller

```bash
# eg, add the domain names to your
echo "<ip to nodeport/ Loadbalancer of Ingress Controller>  www.my-site.com" >> /etc/hosts
echo "<ip to nodeport/ Loadbalancer of Ingress Controller>  www.travel.my-site.com" >> /etc/hosts
echo " <ip to nodeport/ Loadbalancer of Ingress Controller> www.shop.my-site.com" >> /etc/hosts
```

### Create `Ingress Resource`

- Ingress Resource: define rules to route HTTP traffic to Pod or Service (which url to which service)

  - www.my-site.com
  - www.travel.my-site.com
  - www.shop.my-site.com
  - www.my-site.com/docs

- `Ingress Resource` should be created in the same namespace as the services

```bash
k create secret tls testsecret-tls --cert=path/to/tls.cert --key=path/to/tls.key

# apiVersion: v1
# kind: Secret
# metadata:
#   name: testsecret-tls
#   namespace: default
# data:
#   tls.crt: base64 encoded cert
#   tls.key: base64 encoded key
# type: kubernetes.io/tls
```

- create Ingress in cli

```bash
kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80
```

```yml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /   # rewrite url path to "/" in the target service,
    # see ref https://kubernetes.github.io/ingress-nginx/examples/rewrite/
spec:
  tls:  # use https by providing a Secret that contains a TLS private key and certificate
  - hosts:
      - wear.my-site.com
      - travel.my-site.com
    secretName: testsecret-tls
  rules:
    - host: wear.my-site.com # route by subdomain
      http:
        paths:
          - path: / # route by url path
            pathType: Prefix
            backend: # direct to port 80 of shop-service
              service:
                name: shop-service
                port:
                  number: 80
          - path: /testpath
            pathType: Prefix
            backend:
              service:
                name: test
                port:
                  number: 80
          - path: /watch
            pathType: Prefix
            backend:
              service:
                name: watch-service
                port:
                  number: 80
    - host: travel.my-site.com # route by subdomain
    ...
```

# Network Policies (netpol): Restrict Inter-pod Communication

- [docs](https://kubernetes.io/docs/concepts/services-networking/network-policies/)

- by default Kubernetes use a `all allow rule`:

  - allow traffic from any pod to any other pod or services within the cluster

- Network Policies can restrict these traffic

  - Network Policies is `attached to pods`,

    - if attached and set policyTypes: - Ingress, all the Ingress will be restricted, excepts those allowed by rules
    - Ingress traffic in rules will be allowed
    - Egress traffic will not be affected
    - if set policyTypes: - Egress, same rule

  - Network Policies rules define Ingress/Egress' `allow targets (whitelist)`
    - in a `ipBlock`
    - in a `namespace`
    - with a `label`
  - Network Policies define `ports that allows` for Ingress/Egress

- Network Policies are enforced by the network solution implemented on Kubernetes cluster

  - Kube-router, Calico, Romana, Weave
  - Flannel does not support Network Policies

- !! Note that Ingress/Egress are only for request traffic. All response to a request are allowed naturally if the initial request go through

```yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector: # find pods that applies: use label to
    matchLabels:
      role: db
  policyTypes:
    - Ingress
    - Egress
  ingress: # rule apply to the pods
    - from: # multiple items (rules) in a policy is a OR rule
        - ipBlock: # 1. use cidr (ip blocks)
            cidr: 172.17.0.0/16
            except:
              - 172.17.1.0/24
        - namespaceSelector: # 2. use namespace
            matchLabels:
              project: myproject
        - podSelector: # 3. use labels
            matchLabels:
              role: frontend
        - ipBlock: # if multiple items are place in a single rule, it is a AND rule
            cidr: 172.17.0.0/16
            except:
              - 172.17.1.0/24
          namespaceSelector: #
            matchLabels:
              project: myproject
      ports: # ports in "this" pod
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:
            cidr: 10.0.0.0/24
      ports: # ports in the destination
        - protocol: TCP
          port: 5978
```
