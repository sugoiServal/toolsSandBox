# AWS cloud storage overview
![](https://imgur.com/k1ngXeq.jpg)
# Amazon S3
- `"infinitely scaling"` storage, used for:
    - backup and storage
    - disaster recovery
    - Archive and restore
    - Hybrid Cloud storage (storage in both cloud and premise)
    - Media hosting
    - Data lake & Big data analytics
    - website or software delivery
### Durability vs Availability
- Durability: how rare you are expected to lose objects stored in S3(extremely rare)
- Availability: measures how readily an object can be retrieved
    - varies depending on the storage class
### Shared Responsibility Model

|AWS||
|-|-|
|Infrasturucutre|S3 Verrsioning|
|Config, Vulnerability analysis|S3 Bucket Policies|
|appliance validation|S3 Replication Setup|
||S3 logging and monitoring|
||S3 Storage classes|
||Data encryption|
## Bucket and object
- S3 stores `object` (files) in `bucket` (dir/collection of object)

### Object
- Object are access by `KEY`, which is the objects' full path, Object itself, is a `Value`
    - bucket name: my-bucket
    - prefix: my_folder1/another_folder
    - object name: my_file.txt 
> KEY: s3://my-bucket/my_folder1/another_folder/my_file.txt 
- Object Size:
    - Max object size is 5TB
    - for Object that is larger than 5GB, must use "multi-part upload" of 5GB
- `Meta data` to objects:
    - list of textauthentication key/value pairs
    - Tags
    - version ID...
### Bucket
- bucket name must be globally unique:
    - unique across all `region` and all `account`
    - conventions:
        - NO uppercase, underscore, NOT `IP address`, NOT start with prefix `xn--`, suffix `-s3alias`
        - Start with lowercase or number
        - 3-63 char long
- buckets are defined at `region level`, must be created in a region

## S3 Storage Classes and price

|tier|purpose|cost|availability|use case|
|-|-|-|-|-|
|`S3 Standard`| for `frequently accessed` data||99.99|general purpose business data|
|`S3 Intelligent-Tiering`| for `automatic cost savings` for data with `unknown access patterns`,|monitoring and auto-tiering fee + price based on access freq |99.9|auto tiering|
| `S3 Standard-Infrequent Access (S3 Standard-IA)`| for `less frequently` accessed data, but requires rapid access when needed|less than Standard + retrieval cost|99.9|backups|
| `S3 One Zone-Infrequent Access (S3 One Zone-IA)`| for `less frequently` accessed data, but requires rapid access when needed, only one copy in one AZ: `AZ gone data gone`|less than Standard + retrieval cost|99.5|secondary backups|
| `S3 Glacier Instant Retrieval`| for archive data that needs `immediate access`, Min store duration:90 days|price for store + retrieval cost|in ms|archive|
|`S3 Glacier Flexible Retrieval` |for rarely accessed long-term data that does `not require immediate` access, Min store duration:90 days| price for store + retrieval cost|Expedited(1-5min), Standard(3-5min), Bulk(5-12h)|archive|
| `S3 Glacier Deep Archive` | for long-term archive and digital preservation with `retrieval in hours` at the lowest cost storage in the cloud. Min store duration:180 days| price for store + retrieval cost|Standard(12h), Bulk(48h)|archive|
| `S3 Outposts`| If you have data residency requirements that canâ€™t be met by an existing AWS Region, `S3 Outposts` store your S3 data on premises.|

> upload object to bucket -> properites -> Storage class

- change storage class
> object -> properites -> Storage class

### `Lifecycle rules`:
- Allows automate object's Storage Class based on storage access interval
> bucket -> Management -> Lifecycle rules

## S3 Security
S3 Security includes:
- Resource-based:
    - bucket policies: `most common`, allow cross account access to bucket
    - object access control list(ACL): finer grain (can be disabled)
    - bucket access control list(ACL): `least common` (can be disabled)
- Encryption: `encrypt S3 objects` using encryption keys

### Accessing S3 resource
- WWW public access/ other account: use Bucket Policy
- User in the accout: use IAM policy
- AWS services, eg: EC2 instance: use IAM role

> an identity can access S3 object if:
> ( IAM permissions allow OR resource policy allows) AND there's no explicit DENY

### `Bucket Policy`
- a json that define access control
    - Effect: Allow/Deny
    - Principal: target, user or account.
    - Action: Set of APIs
    - Resource: key to the resource

![](https://imgur.com/nPZorJJ.jpg)

- Bucket Setting for Block Public Access:
    - if the setting `block all public access` is on, bucket content will not be public even though `Bucket Policy` might allow
    - `block all public access` set on account level

### Encryption
Server-side Encryption vs client-side encryption
![](https://imgur.com/1EKqyrX.jpg)

## Misc
### Static website hosting
> bucket -> properties -> Static website hosting

### Bucket versioning
- Allow `assigning new version` to object when `overwrite to same object key` occurs
    - Easy roll back to prev versions, protect against unintended delete

- `Versioning` is enabled at the `bucket level`
- misc:
    - files uploaded before enabling versioning have version ID: null

>  buckets -> properties -> Bucket Versioning

### S3 Replication
- Replication allow `sync objects and version in different buckets(region/accounts)`
- `cross region` replication(`CRR`) / `same region` replication(`SRR`)
- Replication properties;
    - Must enable `Versioning` in source and destination to replication
    - Must give proper `IAM write/read permission` in source and destination to replication
    - Replication can be in different account. Copying is async(behind the scene)

- use cases:
    - CRR: compliance; low latency access; replication across accounts
    - SRR: log aggregation, replication between production and test accounts
- [demo](https://www.udemy.com/course/aws-certified-cloud-practitioner-new/learn/lecture/20207498#overview)
>  bucket -> management -> replication rules -> create replication rule


# AWS Snow Family - data migration/edge computing

## Data migration

- Snow Family Data Migration servide: 
    - AWS sends you a physical storage device(`AWS Snowball`) via post office, 
    - You connect it to your server, load your data into it using an client software.
    - After done send it back to AWS. 
    - AWS then help you transfer data into the cloud. 

- why: save you local bandwidth, utilize AWS super fast bandwidth 
    - Pay per data transfer job

### Snowball Edge
- use case: large data cloud migrations
- options:
    - 80TB of HDD capacity
    - 42TB of HDD capacity
### Snowcone & Snowcone SSD
- Light comparing to Snowball
- options
    - 8TB HDD
    - `14TB` SSD
### AWS Snowmobile
- Transfer 100 PB =  100000 TB data
- provide you a truck to transport 

## Edge Computing
- process data while it is being created on an `edge location`

- `edge location`: place where data can be produced but cannot be processed: 
    - no Internet access
    - no computing power...

- support devices:
    - `snowball edge`, up to 80 TB
    - `snowcone` , up to 14 TB
- How
    - we setup a `snowball edge` or `snowcone` device, Both `with local computation power`, in `edge location` 
    - All of the devices `can run EC2 and AWS Lambda`
    - After done, send it back to AWS, AWS wipe out everything
- use cases:
    - preliminary data processing
    - Edge Machine Learning
    - Transcoding media stream
- Options
![](https://imgur.com/KizWIqu.jpg)

### AWS OpsHub
- remote `GUI client` to use with `Snow Devices`

# AWS Storage Gateway 
- AWS Storage Gateway is Hybrid Cloud Storage tool
    - it bridge whatever happen on premises data into Cloud data
    - `bridge your file systems and your storage on-premises into the cloud to leverage best of both worlds`

- `Hybrid Cloud Storage`: part of the  storage infrastructure on cloud, and part on premise
- Reason: 
    - Security requirement
    - Compliance requirement
    - IT strategy...