- Go is designed to be concurrent:

  - write concurrency safely using simple syntax:
    - Goroutines
    - Channels
    - Mutex...

- Go encourage Channels over Mutex

# Goroutines

- `go` keyword spawns a go routine

- think `Goroutines` a `separate thread of execution` to the rest of the code

- `Goroutines` are managed by the `Go runtime`,
  - Goroutines are spawned and destory easily
  - Go runtime manages Goroutine scheduling, deciding when and how to run Goroutines

```go
// Goroutines runs concurrently with the calling function and other Goroutines
go doSomething()

// pack a block of concurrent code in anonymous func
go func() {
  // do something
}()
```

- Communication: Goroutines communicate and synchronize using channels

### More on Goroutines

- Goroutines in Go are concurrent, not necessarily parallel.

  - concurrent: multiple tasks can be switched when block happens (maybe one works, or more)
  - parallel: many workers

- Goroutines is `concurrent` even in single-core machine

- Go provides the `runtime.GOMAXPROCS(n)`: control the number of OS threads that execute Go code simultaneously.

  - `default` sets to the number of CPU cores available
  - Go's runtime will distribute goroutines across multiple CPU cores automatically
  - in this sense, if the OS is multi-core, Goroutines run `parallel` by default

- `Goroutines in Go are not directly mapped to system threads`. Instead, goroutines are a higher-level abstraction `managed by the Go runtime`
  - `Operating System Threads`: Go manages a small number of operating system threads. (aka, "OS threads" or "green threads.") typically equal to the number of CPU cores available.
  - `Goroutines`: Go runtime schedules and multiplexes many goroutines onto this limited number of OS threads.
    - comparing to actual system thread, Goroutines are much `lightweight, more efficient` to create/delete
    - you can create even millions of goroutines without significantly impacting performance.
  - `Scheduler`: scheduler is responsible for distributing the execution of goroutines onto OS threads. Like switching between goroutines and make sure CPU resource are well used.
    - eg. if a goroutine performs a blocking operation, the Go runtime can automatically move that goroutine to another OS thread to ensure that other goroutines continue to make progress

# Channels: communication, resync

- Channels are concurrent `message broker`

  - think about activeMQ/RabbitMQ in the scope of parallel computing
  - Producer/ Consumer

- a channel is `essentially a thread safe (avoid race conditions) queue`.
  - we have different Goroutines as producer/consumer
  - sender routines push to the end of the queue
  - consumer routines pull from the start of the queue
- Channels have `ownership`,
  - they are typically `created (and closed) by one goroutine `
  - then `passed to other goroutines for use`.
  - the creator goroutine to control access to the channel.

## `Syntax`

- `create a channel` with `make(chan)`
- `<-` operator: send data and receive data.
- `direction of the arrow` indicates the `direction of data`
- `ch <- data`: Sends data into the channel
- `data := <-ch`: Receives data from the channel.
- `Closing Channels`: A channel can be closed to signal that no more data will be sent on it (and consumers should stop receiving)

```go
ch := make(chan int)
go func() {
    ch <- 42 // Send data to the channel
}()
result := <-ch // Receive data from the channel
close(ch)
```

### Channels is treated as Collection type

- The `select` statement allows you to listen to multiple channels simultaneously.
  - whenever there is a available data from any channel, the `case` will be triggered
  - `default (optional)`: if default is used, default will be triggered when `all channels are blocked` (no data ready from any of the channels)

```go
select {
case data, ok := <-ch1:
    if !ok {return}
    // Handle data from ch1
case data, ok := <-ch2:
    if !ok {return}
    // Handle data from ch2
case ch3 <- value:
    // Send value into ch3
default:
    // do something else if no data
}
```

- `loop` through a `range of channel` is possible
  - blocking at each iteration if nothing new is there
  - loop `exit` only when the channel is `closed`.

```go
for item := range ch {
    print(item)
    // for loop is blocked until a new item received from ch
}
```

### Unidirectional Channels

- A Channels can be declared as send-only (chan<-) or receive-only (<-chan) inside a function signature

  - to prevent unintented channel usage in function

```go
// Producer Function:
// Send-Only Channel: cannot receive data from the channel
func sendData(ch chan<- int, data int) {
    ch <- data
}

// Consumer Function:
// Receive-Only Channel: cannot send data to the channel
func receiveData(ch <-chan int) int {
    return <-ch
}

// Bidirectional Channel
func processChannel(ch chan int) {}
```

### `Closing` Channel

- A channel is generally close in the same scope where it is made.
- Consumer of a channel can `check if the channel is closed`.

- closed channel behavior
  - `receive from a closed` channel return zero-value
  - `Sending to a closed` channel will cause `panic`

```go
// Explicitly close a channel by producer.
ch := make(chan int)
// do producer things
close(ch)

// consumer should chech if a channel is closed
func consumer(ch chan int) {
    for {
        msg, ok := <-ch
        if !ok {
            // Channel closed, exit the consumer
            return
        } else {
            processMsg(msg)
        }
    }
}
```

### Nil Channel

- send/receive to a Nil channel block forever

```go
var c chan string // c is nil
c <- "let's get started" // blocks
fmt.Println(<-c) // blocks
```

## Channel Behaviors

### `Blocking`

- Channel operations are `blocking`

```go
// (only Unbuffered Channel) If there is no consumer to ch, the next code will be blocked
ch <- 69
print("hello")


// If there is no producer to ch, the next code will be blocked
v := <- ch
print("hello")

// this create a deadlock: producer and consumer are the same goroutine
func filterOldEmails(emails []email) {
  ch := make(chan bool)
  for _, e := range emails {
    if e.date.Before(time.Date(2020)) {
      ch >- true
      continue
    }
    ch <- false
  }
  isOld := <- ch
  fmt.Printf("email 1 is old: %v", isOld)
}

// spawning a routine solve the deadlock
func filterOldEmails(emails []email) {
  ch := make(chan bool)
  go func(){
    for _, e := range emails {
      if e.date.Before(time.Date(2020)) {
        ch >- true
        continue
      }
      ch <- false
    }
  }

  isOld := <- ch
  fmt.Printf("email 1 is old: %v", isOld)
}
```

- example: wait(block) server startup until all databases are online

```go
import (
	"fmt"
	"time"
)

func waitForDbs(numDbs int, dbCh chan struct{}) bool {
	for i := 0; i < numDbs; i++ {
		<-dbCh      // the code will stop until all db connect tokens are received
	}
	return true
}

func connectDbs(numDbs int) chan struct{} {
	ch := make(chan struct{})
	go func() { // simulate db connection
		for i := 0; i < numDbs; i++ {
			time.Sleep(1 * time.Second)
			ch <- struct{}{} // empty struct are used as token that indicate a db is connected
			fmt.Printf("Database %v is online\n", i+1)
		}
	}()
	return ch
}

func main() {
	dbCh := connectDbs(5)
	if isConnected := waitForDbs(5, dbCh); isConnected {
		fmt.Println("All Database are online")
	}
}
```

### `Buffer`

- There's a `buffer` to each of the channel, The `buffer size` can be specified when `make` the channel.
  - For a buffered channel, producer can send message to the channel `even there is no consumer`, until the `limit of the buffer` is reached, then `the producers will be blocked`.
- `buffer` is ordered, that's why channels are queue

- example: `inside the same thread`, producer(addEmailsToQueue) batching a number inside channel before the consumer(sendEmails) process it

```go
import (
	"fmt"
)

func addEmailsToQueue(emails []string) chan string {
	emailsToSend := make(chan string, len(emails))
	for _, email := range emails {
		emailsToSend <- email
	}
	return emailsToSend
}

func sendEmails(batchSize int, ch chan string) {
	for i := 0; i < batchSize; i++ {
		email := <-ch
		fmt.Println("Sending email:", email)
	}
}

func main() {
	emails := []string{"She says hi!", "Yeah its tomorrow. So we're good.", "Cool see you then!", "Bye!"}
	fmt.Printf("Adding %v emails to queue...\n", len(emails))
	ch := addEmailsToQueue(emails)
	fmt.Println("Sending emails...")
	sendEmails(len(emails), ch)
}
```

# `Sync Package`: Built-in Concurrency Primitives

- the [sync package](https://pkg.go.dev/sync)

  - Mutex
    - Lock()
    - Unlock()
  - RWMutex (writer Mutex)
    - RLock()
    - RUnlock()
    - Lock()
  - WaitGroup
    - Add()
    - Done()
    - Wait()
  - Cond
    - Wait()
    - Signal()
    - Broadcast()
  - Once
    - Do()

- Resource Locks

  - `Mutex`

    - used to protect shared resources from concurrent access,
      - only one goroutine can access the protected resource at a time

  - `RWMutex`
    - when the resource is read-prone and less written, RWMutex allows:
      - read resource simultaneously
      - only one goroutine can write at a time

- Coordinate(Resync)

  - `WaitGroup`

    - used for waiting for a collection of goroutines to finish executing before proceeding (re-SYNC)

  - `Cond`

    - allows one or more goroutines to wait for a specific condition to become true before proceeding

  - `Once`
    - used to ensure that a specific function is executed only once, even if it is called from multiple goroutines.
      - often used for one-time initialization tasks.

## Resource Locks

### Thread-Safe (or Atomic)

- `a non Thread-Safe resource` meaning they can lead to data `race condition` if accessed concurrently by multiple goroutines without proper control

- In Go, several resources are not inherently thread-safe

  - `Maps`
  - `Slices`
  - `Custom Data Structures`: unless you explicitly design to be thread-safe

- Non Atomic Operations

  - +=. -=, ++, --
    - use sync/atomic package for these operations like `atomic.AddInt64` or `atomic.LoadInt32`.
  - `Closing a channel`
    - do not close a channel from one goroutine while another goroutine is still sending or receiving data from it
    - may lead to panic or undefined behavior
  - `File Operations`: like os.Open, os.Create
  - `Network Connections`
  - `Pointer Dereferencing`: Accessing and modifying data through pointers is not thread-safe

- when access the above resource from multiple goroutines, you want to use `Mutex` or `RWMutex`

### Mutex

- Mutex: Mutual Exclusion
- Mutex interface

  - mu.Lock(): when in effect, any other calls to `mu.Lock()`, or `RLock()` will block
  - mu.Unlock()

- Use Mutex:

  - create Mutex
  - distribute Mutex to goroutines that CRUD to shared resources
  - Lock() upon accessing the resource
  - Code wrapped inside Lock() and Unlock() is protected
  - Unlock() after finish accessing the resource

- example:
  - two goroutines add 1 to counter 100000 times,
  - two goroutines minus 1 to counter 100000 times,
  - expect: cpimter result: 0
  - if Mutex not used, the result is unpredictable

```go
package main

import (
	"fmt"
	"sync"
)

func main() {
	var mu sync.Mutex   // create Mutex
	var counter int

	var wg sync.WaitGroup
	const numGoroutines = 2

	for i := 0; i < numGoroutines; i++ {
		wg.Add(1)
		go func() {
			for j := 0; j < 100000; j++ {
				// Lock the mutex before incrementing the counter.
				mu.Lock()
				counter++
				println(counter)
				// Unlock the mutex when done.
				mu.Unlock()
			}
			wg.Done()
		}()
	}
	for i := 0; i < numGoroutines; i++ {
		wg.Add(1)
		go func() {
			for j := 0; j < 100000; j++ {
				// Lock the mutex before incrementing the counter.
				mu.Lock()
				counter--
				println(counter)
				// Unlock the mutex when done.
				mu.Unlock()
			}
			wg.Done()
		}()
	}

	// Wait for all goroutines to finish.
	wg.Wait()

	fmt.Println("Counter:", counter)
}
```

### RWMutex

- RWMutex allows multiple readers to a `RLocked resource`

  - make to application run faster by allowing concurrent read in a read-heavy scenerio

- RWMutex interface
  - Work exactly the same as Mutex
    - mu.Lock()
    - mu.Unlock()
  - RLock(): when in effect, any other calls to `mu.Lock()` will block, BUT, other calls to `mu.RLock()` will not block
  - RUnlock()

```go
type safeCounter struct {
	counts map[string]int
	mu     *sync.RWMutex
}

// Lock writer
func (sc safeCounter) inc(key string) {
	sc.mu.Lock()
	sc.slowIncrement(key)
	sc.mu.Unlock()
}

// RLock reader
func (sc safeCounter) val(key string) int {
	sc.mu.RLock()
	defer sc.mu.RUnlock()
	return sc.counts[key]
}
```

## Coordinate(Resync)

### WaitGroup

- `WaitGroup`: provides a way to `wait for a collection of goroutines to finish` their work before the program to proceed.
  - Add(delta int)
    - increments the WaitGroup's counter by delta.
    - `call Add(1) just before starting a goroutine` that you want to wait for
  - Done()
    - Done() decrements the WaitGroup's counter by 1
    - `call Done() at the end of each goroutine's execution` to signal that it has completed its work.
  - Wait()
    - The Wait() blocks the program until the WaitGroup's counter becomes zero

```go
func worker(id int, wg *sync.WaitGroup) {
    defer wg.Done() // Decrement the counter when the goroutine is done.
    fmt.Printf("Worker %d is starting...\n", id)
    time.Sleep(time.Second)
    fmt.Printf("Worker %d is done.\n", id)
}

func main() {
    var wg sync.WaitGroup

    for i := 1; i <= 3; i++ {
        wg.Add(1)       // Increment the counter before starting a goroutine.
        go worker(i, &wg)
    }

    wg.Wait()           // Wait for all goroutines to finish.

    fmt.Println("All workers have finished.")
}
```

### Cond(TODO)

### Once(TODO)
