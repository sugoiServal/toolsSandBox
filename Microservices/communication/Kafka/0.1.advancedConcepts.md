# Architect

- Kafka Client (APIs, process written in any language)

  - `producer`: push record to topic (through broker)
  - `consumer group`
    - `consumer`: pull record from topics (through broker)

- broker

  - node in Kafka cluster, run Kafka process
  - distribute store topic (`partition`, `replicas`)
  - stateless, the state of cluster: maintained with ZooKeeper/Kraft (metadata: topics, partitions, consumer group, leaders...)
  - 100,000/s capability per node

- topic:

  - topic is abstract, a topic is a `collection of partition`,
  - partitions
    - partitions are distributed among brokers
    - a broker can have many partitions, and may from different topics
    - partitions can have `replicas` (followers, promotion on leader failure), replicas are also distributed among brokers

- controller

  - kafka 在启动时会从所有的 broker 中选择一个作为 controller (controller node)。
    - other broker create a listener to the controller node
    - if a controller fail, other broker will become the new controller
  - responsibility：
    - create topic, change parition/replicas number
    - elect partition leader

- 消息的语义性
  - At-most once: 管 consumer 收不管 consumer 是否成功，可能丢失数据
  - At-least once: 可能重复消费
  - Exactly once: 最佳，保证事务仅被处理一次

### Replicas

- `leader-follower`: when replicas enabled, each parition has

  - `leader` partition (must be 1): only leader do read and write! read from consumer, write from producer.
  - `follower` partitions (0 or multiple replicas): follower don't read or write. follower sync from leader, and promoted as leader when leader fail

- when we refers to `a partition` in kafka, we generally refers to the `leader partitions`

- ar，isr，osr

  - Assigned Replicas(ar): all replicas
  - In-Sync Replicas(isr): replicas currently in-sync with the leader
  - Out-of-Sync Replicas(osr): replicas currently Out-of-Sync with the leader. 正常运行情况下不应该存在 OSR

- leader election

  - new leader is elected from isr, controller node determine the leader election
  - rule:
    - controller 读取 isr, 并选取任意一个 replica 作为 leader
    - 如不存在 isr, 新的 leader 为-1

- rebalancing a partition's leader within nodes
  - 某个 broker crash 后可能导致 partitions leader 分布不均衡。例如: 一个 broker node 上有很多 leader
  - 下面指令可以重新分配 leader， rebalancing leader 在 broker 中的分布

```bash
kafka-leader-election.sh --topic quickstart-events  --bootstrap-server kafka-cluster-kafka-0-1:9092 --partition=0 --election-type preferred
```

## Producer

- producer write data flow:
  - producer determine the topic and partition based on the given conf
  - producer find leader partition from KRaft/Zookeeper
  - record sent to leader, leader write record to local log
  - follower pull the same record from leader, write to local log, send ack to leader
  - leader return acks

### `idempotence`

- [video](https://www.bilibili.com/video/BV19y4y1b7Uo?p=12)

- idempotence: a property of operations，such that no matter how many times the operation is executed (eg, submit), the result is the same

- enable.idempotence: ensure idempotence, avoid that a producer retries and re-sends the same record multiple time
  - enable.idempotence will slow down the pruduce throughput
  - `acks=all` + `enable.idempotence=true` => maximum durability and guarantee ordered in producer side

```java
props.put("enable.idempotence", "true");
```

### `acks`

- acks is related replicas
- controls what level of acknowledgments the producer requires from the broker after sending a message, before considering the message as "sent" or "committed."
  - acks=0: best efficiency: does not require any acknowledgment from the broker
  - acks=1: balanced: require acknowledgment from the leader partition.
  - acks=all: best durability: requires acknowledgment from all in-sync replicas (ISR) of the partition.

```java
// acks=0: does not require any acknowledgment from the broker: best efficiency
// acks=1: require acknowledgment from the leader partition. balanced
// acks=all: requires acknowledgment from all in-sync replicas (ISR) of the partition. best security
props.put("acks", "all");  // or -1
props.put("acks", "1");
props.put("acks", "0");
```

### `partitioner`

- [video](https://www.bilibili.com/video/BV19y4y1b7Uo?p=13)
- `partitioner`: deciding partition number (in a topic) for each message
- partitioner strategies:
  - api: `ProducerRecord(String topic, Integer partition, K key, V value)`
  - hard-code: when partition is not null: If a producer specifies a partition number in the message record, use it.
  - key-based: when key is provided: If the message provides a key，choose a partition based on a hash value of the key
    - when partition number is fixed, guarantees that same key always send to the same partition. In this case records within a key is `ordered`.
    - when partition number increased/decreased, that same key will be sent to another paritition
    - doesn't ensure that two different keys will never have the same partition
    - if a key contains too many records, partitions would be inbalanced
  - round-robin: when key and partition are null: pick a partition in a round-robin fashion. Records is `not ordered`.
  - custom partitioner: [implement the Partitioner Interface](https://www.learningjournal.guru/courses/kafka/kafka-foundation-training/custom-partitioner/)

```java
props.put("partitioner.class", "CustomPartitioner");
//...
public class CustomPartitioner implements Partitioner {

    public void configure(Map<String, ?> configs) {
        r = new Random();
    }

    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {

        int numPartitions = cluster.partitionCountForTopic(topic);
        return r.newtInt(1000) % numPartitions;
    }

    public void close() {
    }
}
```

## Consumer

- consumer and broker are both stateless. all state information (offsets) are stored inside zookeeper/kraft

- consumer consume data flow:
  - get partition through partition assignment strategy
  - get partition leader node and offset from zookeeper/kraft
  - poll data from partition leader
  - commit new offset to zookeeper/kraft

### consumer group

- consumer group

  - consumers with the same `group.id` belongs to the same group
  - `Partitions are not shared` - at any given time, `each partition` of a topic is consumed by only `one consumer`
  - each consumer can consumer
    - multiple topics,
    - multiple partitions
  - partitions and consumer: `Many to One relation`

- `group coordinator`: for each group, `one Kafka broker is elected automatically`, responsible for

  - managing group membership: maintaining a list of active group members and their partition
  - detect state change and `trigger group leader to rebalance`

- `group leader`: The `first consumer` to participate in a consumer group becomes a group leader, responsible for

  - `rebalance` (partition re-assignment)
  - communicate rebalance result to group coordinator

- rebalance:

  - may triggered by:

    - number of consumer change (add/remove)
    - number of partitions change (add/remove)
      - eg: topics consumers subscribe to were deleted

  - rebalance 过程中所有消费者都将停止工作，直到 rebalance 完成

- partition-consumer assignment strategies [讲解](https://www.bilibili.com/video/BV19y4y1b7Uo?p=15)
  - Range(default): 对于 topic, 可确保每个消费者消费的 partition 数量是均衡的
  - Round-Robin: 可确保每个消费者消费的 partition 数量是均衡的
  - Sticky: 确保在 发生 rebalance 后的 partition 尽可能与 rebalance 前相同。没有 rebalance 时与 Round-Robin 类似

```java
props.put("partition.assignment.strategy", "org.apache.kafka.clients.consumer.RoundRobinAssignor");
props.put("partition.assignment.strategy", "org.apache.kafka.clients.consumer.RangeAssignor");
props.put("partition.assignment.strategy", "org.apache.kafka.clients.consumer.StickyAssignor");
```

- in production: set the # of partitions according to the # of available consumers for maximum efficiency

### offset

- offset:

  - offset is relative to a `partition`: represents the last consumed message's location in the partition.
  - offsets' records are stored inside a special internal Kafka topic: `__consumer_offsets`. consumers retrieve offset from the topic
  - avoid process the same record multiple times
    - when consumer-partition ownership change, it tell the new consumer where to start
    - If a consumer fails and rejoins with the same partition (eg, through sticky assignments), it can resume from where it left off.

- there are two types of offsets:

  - `Current offset`: a pointer to the last record that a partition has already sent
    - increment `when a partition sent` the most recent poll to a consumer
  - `Committed Offset`: a pointer to the last record that a consumer has successfully processed
    - increment when a consumer has processed a record and then `commit the offset`

- two ways to `Committed Offset`
  - Auto commit(default): may cause second processing of records
  - Manual-commit:
    - Commit Sync: blocking until commit is acknowledged or or until an error occurs (guarantee process exact once)
    - Commit async: non-blocking, send commit without waiting or retry (not guarantee process exact once)

> Let us understand it with an example.
> You have some messages in the partition, and you made your first poll request. You received 10 messages hence the consumer increases the current offset to 10. You take four seconds to process these ten messages and make a new call. Since you haven't passed five seconds, the consumer will not commit the offset. You received another set of records, and for some reason rebalance is triggered at this moment. First ten records are already processed, but nothing is committed yet. Right? The rebalance is triggered. So, the partition goes to a different consumer. Since we don't have a committed offset, the new owner of partition should start reading from the beginning and process first ten records again.
> You might be thinking that let's reduce the commit frequency to four seconds. You can lower the incidence of commit by setting the auto-commit interval to a lower value, but you can't guarantee to eliminate repeat processing.

- Auto commit

```java
// auto commit

// automatically commit offset
props.setProperty("enable.auto.commit", "true");
// interval of automatically commit offset
props.setProperty("auto.commit.interval.ms", "1000");
    //  If 1000 ms have passed since the previous call, the consumer will auto commit the last offset
```

- manual commit

```java
props.put("enable.auto.commit", "false");

try {
    consumer = new KafkaConsumer<>(props);
    consumer.subscribe(Arrays.asList(topicName));

    while (true) {
        ConsumerRecords<String, Supplier> records = consumer.poll(100);
        for (ConsumerRecord<String, Supplier> record : records) {
            System.out.println("Supplier id= " + String.valueOf(record.value().getID()) +
                    " Supplier  Name = " + record.value().getName() +
                    " Supplier Start Date = " + record.value().getStartDate().toString());
        }
        consumer.commitAsync();
    }
} catch (Exception ex) {
    ex.printStackTrace();
} finally {
    consumer.commitSync();
    consumer.close();
}
```

# Physical Storage (TODO)

- [physical storage](https://www.bilibili.com/video/BV19y4y1b7Uo/?p=24)
- [persistent](https://www.bilibili.com/video/BV19y4y1b7Uo/?p=25)
- [data clean](https://www.bilibili.com/video/BV19y4y1b7Uo/?p=26)

- store direction: distribution dependent

  - /bitnami/kafka/data (bitnami kafka distro)

- topic

  - partitions
    - segments (最小的数据文件). segment 文件名：offset location where segments started
      - .log: record data
      - .index: index records for offset query, .
      - .timeindex: index based on time

- 概念:

  - LEO (log-end-offset): producer 写入的 log 末端 offset
  - partition offset, segment offset(local)

## Write/Read to segment

- when consumer start to consume a record:
  - consumer have partition offset (全局)
  - find segment through partition offset
  - transform partition offset to segment offset
  - find record by querying .index with segment offset

### Configs

- segment size limit: 规定每个 segment 文件最大为 10M

```bash
kafka-topics.sh --create --topic segment_10m --partitions 3 --replication-factor 3 --bootstrap-server kafka-cluster-kafka-0-1:9092 --config segment.bytes=10485760
```

## Log Clean

# Listener

- ref
  - [api docs](https://kafka.apache.org/31/generated/kafka_config.html)
  - [docs](https://kafka.apache.org/documentation/#listener_configuration)
  - [blog](https://rmoff.net/2018/08/02/kafka-listeners-explained/)
- issue:

  - kafka broker run inside docker network/local network.
  - Client(producer/consumer) from external host can resolve and connect to a bootstrap server in the network successfully .
  - But the metadata (leader node) return from bootstrap server cannot be resolved correctly
  - So external client cannot send anything to broker unless that the broker is exactly the bootstrap server

- `kafkacat` is a useful tool for exploring this issue
  - -L you can see the metadata for the listener to which you connected.

```bash
kafkacat -b ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 -L
```

- listeners:
  - `KAFKA_LISTENERS`: localhost ports used by the listener process
    - comma-separated list of listeners' name (name://host:port)
    - default (when a host is not given ) is localhost 0.0.0.0
  - `KAFKA_ADVERTISED_LISTENERS`: Listeners's metadata to be return for clients to connect
    - comma-separated list of listeners name (name://host:port)
    - If this is not set, the value for listeners will be used
  - `KAFKA_LISTENER_SECURITY_PROTOCOL_MAP`
    - comma-separated list of key/value pairs (key:value)
    - key: listener name. value: security protocol (eg. PLAINTEXT, SSL)
  - KAFKA_INTER_BROKER_LISTENER_NAME: Name of listener used for communication between brokers
    - host/IP used must be accessible from other broker in the local network
  - KAFKA_CONTROLLER_LISTENER_NAME: names of the listeners used for communication between KRaft controller. This is required if running in KRaft mode
