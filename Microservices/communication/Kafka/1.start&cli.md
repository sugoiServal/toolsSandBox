- resources

  - [quickstart](https://kafka.apache.org/documentation/#quickstart)
  - [image](https://hub.docker.com/r/bitnami/kafka)
  - [manually build cluster](https://www.bilibili.com/video/BV19y4y1b7Uo?p=4)

- API list:
  - `Admin API`: manage and inspect topics, brokers, and other Kafka objects.
  - `Producer API`: publish stream of events
  - `Consumer API`: subscribe, read stream of events
  - `Kafka Streams API`: higher-level functions to stream processing (transformations, aggregations)
  - `Kafka Connect API`: build and run reusable data import/export connectors (from/ to external system)

# TLDR

- Create Cluster
- 创建 `topic` (logical channel of data)
- (test) `producer` push `record` to `topic`
- (test) `consumer` pull `record` from `topic`
- benckmark

### Create Cluster

```bash
curl -sSL https://raw.githubusercontent.com/bitnami/containers/main/bitnami/kafka/docker-compose-cluster.yml > ~/kafka-cluster/docker-compose.yml
docker-compose up -d
docker exec --interactive --tty kafka-cluster-kafka-0-1 /bin/bash   # start interactive in node-0
```

### kafka gui

- kafka-ui

```bash
# https://docs.kafka-ui.provectus.io/quick-start/demo-run

docker run -it -d -p 8080:8080 -e DYNAMIC_CONFIG_ENABLED=true --network kafka-cluster_default provectuslabs/kafka-ui
```

### 创建 `topic` (logical channel of data)

- you can create a topic from any broker node in the cluster
- topic creation process typically involves using the Kafka command-line tools or make API calls to Kafka broker
- Once a topic is created, it becomes visible to all nodes (topic metadata will be updated to all brokers)

```bash
# create topic
kafka-topics.sh --create --topic quickstart-events --partitions 3 --replication-factor 3 --bootstrap-server kafka-cluster-kafka-0-1:9092

# list topic
kafka-topics.sh --list --bootstrap-server kafka-cluster-kafka-0-1:9092

# delete topic
kafka-topics.sh --delete --topic java-api-test1 --bootstrap-server kafka-cluster-kafka-0
-1:9092

# describe topic
kafka-topics.sh --describe --topic quickstart-events --bootstrap-server kafka-cluster-kafka-0-1:9092
```

### (test) `producer` push `record` to `topic`

```bash
kafka-console-producer.sh --topic quickstart-events --bootstrap-server kafka-cluster-kafka-0-1:9092
>This is my first event
>This is my second event
# stop the producer client with Ctrl-C at any time.
```

### (test) `consumer` pull `record` from `topic`

```bash
# from any node in the cluster
kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server kafka-cluster-kafka-0-1:9092

# stop the producer client with Ctrl-C at any time.
```

### Cluster benckmark test

- [bili](https://www.bilibili.com/video/BV19y4y1b7Uo?p=6&vd_source=c2cab8b68e17f7406c35e588a940b4ae)

```bash
# create benchmark topic (feel free to tweaking the partitions and replication)
kafka-topics.sh --create --topic perf-test --partitions 1 --replication-factor 1 --bootstrap-server kafka-cluster-kafka-0-1:9092


# producer test
kafka-producer-perf-test.sh --topic perf-test --num-records 5000000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers=kafka-cluster-kafka-0-1:9092,kafka-cluster-kafka-1-1:9092,kafka-cluster-kafka-2-1:9092 acks=1
# --throughput -1: 限流，-1不指定

# consumer test
kafka-consumer-perf-test.sh --broker-list kafka-cluster-kafka-0-1:9092,kafka-cluster-kafka-1-1:9092,kafka-cluster-kafka-2-1:9092 --topic perf-test --messages 5000000 --fetch-size 1048576
# --fetch-size 1048576 每次拉取数据大小
# --messages 总共拉取数据数

```

# Setting up Kafka Cluster

- Kafka Cluster can either started using `ZooKeeper` or `KRaft (Kafka Raft, recommend)`
  - these are distributed coordination and synchronization services

### 文件夹

- root
  - bin: .sh for various functionalities (kafka-server-start, kafka-topics, kafka-console-producer...)
  - config: configuration files (server, zookeeper, kraft...)
  - libs: dependencies (Scala/Java)
  - site-docs：帮助文档

```bash
# download https://www.apache.org/dyn/closer.cgi?path=/kafka/3.6.0/kafka_2.13-3.6.0.tgz
wget https://dlcdn.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz
tar -xzf kafka_2.13-3.6.0.tgz
cd kafka_2.13-3.6.0
ls -al
```

### Manually, Zookeeper

- [manually build cluster](https://www.bilibili.com/video/BV19y4y1b7Uo?p=4)

```bash
cd kafka_2.13-3.6.0
vim server.properties   # setting node configurations
    # 1.更改 broker.id: id of node in the cluster. 每一个node的id必须是unique！
    # 2.设置 log.dirs: where kafka data are stored

# start Kraft cluster service
KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties

# start Kafka server
bin/kafka-server-start.sh config/kraft/server.properties
```

## Dockerized Kafka Cluster

- [image](https://hub.docker.com/r/bitnami/kafka)
- [full doc](https://github.com/bitnami/containers/blob/main/bitnami/kafka/README.md)
- [Kubernetes](https://github.com/bitnami/charts/tree/main/bitnami/kafka)

- cluster
  - [Kraft mode configuration](https://github.com/bitnami/containers/blob/main/bitnami/kafka/README.md#apache-kafka-kraft-mode-configuration)
  - [setup a Kraft cluster](https://github.com/bitnami/containers/blob/main/bitnami/kafka/README.md#setting-up-a-apache-kafka-cluster)

```bash
curl -sSL https://raw.githubusercontent.com/bitnami/containers/main/bitnami/kafka/docker-compose-cluster.yml > docker-compose.yml
docker-compose up -d
```

### Configuration

- modify configuration either through

  - modify docker-compose.yml
  - provide as -e in docker run

```yml
KAFKA_CFG_PROCESS_ROLES: "Comma-separated list of Kafka KRaft roles. Allowed values: controller,broker, controller, broker."
KAFKA_CFG_NODE_ID: "Unique id for the Kafka node."
KAFKA_CFG_LISTENERS: "List of Kafka listeners. If node is set with controller role, the listener CONTROLLER must be included."
KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: "Maps each listener with a Apache Kafka security protocol. If node is set with controller role, this setting is in order to assign a security protocol for the CONTROLLER LISTENER. E.g.: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT." # "In order to configure controllers communications without authentication, you should provide CONTROLLER:PLAINTEXT
```

### Setup Step-by-step

- volume

```yml
# docker-compose
# https://github.com/bitnami/containers/blob/main/bitnami/kafka/docker-compose.yml
# https://github.com/bitnami/containers/blob/main/bitnami/kafka/docker-compose-cluster.yml
```

```yml
version: "2"

services:
  kafka-0:
    image: docker.io/bitnami/kafka:3.6
    ports:
      - "9094:9094"
    environment:
      # KRaft settings
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka-0:9093,1@kafka-1:9093,2@kafka-2:9093
      - KAFKA_KRAFT_CLUSTER_ID=abcdefghijklmnopqrstuv
      # Listeners
      - KAFKA_CFG_LISTENERS=BROKER://:9092,CONTROLLER://kafka-0:9093,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=BROKER://:9092,EXTERNAL://localhost:9094
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,BROKER:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=BROKER
      # Clustering
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
    volumes:
      - kafka_0_data:/bitnami/kafka
  kafka-1:
    image: docker.io/bitnami/kafka:3.6
    ports:
      - "54411:9094"
    environment:
      # KRaft settings
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka-0:9093,1@kafka-1:9093,2@kafka-2:9093
      - KAFKA_KRAFT_CLUSTER_ID=abcdefghijklmnopqrstuv
      # Listeners
      - KAFKA_CFG_LISTENERS=BROKER://:9092,CONTROLLER://kafka-1:9093,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=BROKER://:9092,EXTERNAL://localhost:54411
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,BROKER:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=BROKER
      # Clustering
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
    volumes:
      - kafka_1_data:/bitnami/kafka
  kafka-2:
    image: docker.io/bitnami/kafka:3.6
    ports:
      - "54412:9094"
    environment:
      # KRaft settings
      - KAFKA_CFG_NODE_ID=2
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka-0:9093,1@kafka-1:9093,2@kafka-2:9093
      - KAFKA_KRAFT_CLUSTER_ID=abcdefghijklmnopqrstuv
      # Listeners
      - KAFKA_CFG_LISTENERS=BROKER://:9092,CONTROLLER://kafka-2:9093,EXTERNAL://:9094
      - KAFKA_CFG_ADVERTISED_LISTENERS=BROKER://:9092,EXTERNAL://localhost:54412
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,BROKER:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=BROKER
      # Clustering
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=2
    volumes:
      - kafka_2_data:/bitnami/kafka

volumes:
  kafka_0_data:
    driver: local
  kafka_1_data:
    driver: local
  kafka_2_data:
    driver: local
```
